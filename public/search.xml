<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[机器学习算法系列（2）- 支持向量机]]></title>
    <url>%2F2018%2F01%2F25%2F%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%2F</url>
    <content type="text"><![CDATA[前言这节来讲一讲支持向量机（Support Vector Machine, SVM）这个难啃的骨头。照例，我们从以下路线来梳理整个算法的脉络：模型来源 —&gt; 损失函数 —&gt; 优化损失函数 —&gt; 正则化其实这对应着统计学习方法的三要素： 模型：就像上一篇逻辑斯蒂回归模型中，我们从广义线性模型推出了LR的表达式，我们的目标就是学习出这个表达式形式的模型。无穷多种参数的组合导致假设空间包含了无穷多个模型。 策略：有了模型的假设空间，接着需要考虑的是如何从假设空间中选取最优模型。一般我们用损失函数来度量模型预测错误的程度。我们可以通过尽可能选取损失函数最小的模型来得到最优模型。其中，监督学习中有两个基本策略：经验风险最小化和结构风险最小化。 算法：基于数据集，通过什么样的计算方法，根据学习策略去选取最优的模型。这里的算法一般指最优化算法，目标是通过数值计算方法求解损失函数的全局最优值。梯度下降法就是其中之一。 间隔与支持向量TODO 对偶问题TODO SMO优化算法TODO 核函数TODO 软间隔与正则化TODO 支持向量回归（SVD）TODO 附：拉格朗日对偶性TODO]]></content>
  </entry>
  <entry>
    <title><![CDATA[机器学习算法系列（1）- 逻辑斯蒂回归]]></title>
    <url>%2F2018%2F01%2F21%2F%E6%B5%85%E8%B0%88%E9%80%BB%E8%BE%91%E6%96%AF%E8%92%82%E5%9B%9E%E5%BD%92%2F</url>
    <content type="text"><![CDATA[转载请注明出处：https://zhuyuhe.github.io/2018/01/21/%E6%B5%85%E8%B0%88%E9%80%BB%E8%BE%91%E6%96%AF%E8%92%82%E5%9B%9E%E5%BD%92/ 博主即将开始求职之旅，于是搭了这个博客，将备战春招和秋招的复习笔记记录于此。此系列是对机器学习算法理论的复习，力求搞懂算法的来龙去脉，公式繁多，废话较少。以后有时间会在此基础上进行完善，力求通俗易懂。如有不足之处，还请各位读者指正！ 1.从广义线性模型到sigmoid函数首先，LR是二分类模型，我们假设二分类问题服从伯努利分布。即其概率分布为： P(y;\phi)= \phi^y (1-\phi)^{1-y} = exp(ylog\phi + (1-y)log(1-\phi))=exp(ylog\frac{\phi}{1-\phi} + log(1-\phi))而伯努利分布属于指数蔟分布，即其概率分布可以写成如下形式： P(y;\eta)= b(y)exp(\eta^TT(y)-\alpha(\eta))其中，$\eta$为自然参数。广义线性模型认为输入x与自然参数$\eta$为线性关系，即$\eta = \theta^TX$。对比上述两式可以得到： b(y) = 1T(y) = y\eta = log\frac{\phi}{1-\phi}所以有 \theta^TX = \eta = log\frac{\phi}{1-\phi}得到 \phi = \frac{1}{1+e ^ {-\theta^TX}}这里的$\phi$就是伯努利分布中的P(Y=1|X)而LR中的模型输出 h(x) = E(y|x) = 0 · P(Y=0|X) + 1 · P(Y=1|X) = \frac{1}{1+e ^ {-\theta^TX}}这就推导出了逻辑斯蒂回归模型。也解释了为什么LR要用sigmoid函数，因为我们从广义线性模型出发，推导出的LR模型刚好就是sigmoid函数形式的。并且sigmoid有很多良好的数学性质： 连续可导 值域为（0,1）,给了模型可解释性，即将输出结果解释为对应于该分类的概率 下一节我们将从推导出的lr模型出发，去看看它的损失函数是怎么来的。 2. 从极大似然估计到损失函数给定一组数据，我们需要用这组数据去找到最好的参数$\theta$。我们认为使观测结果（即现有的数据）出现的概率最大的参数$\theta$就是最优的参数。这种思想就是极大似然估计。假设这组数据独立同分布，其联合概率可以写成各样本出现概率的乘积。即 L(\theta) = \prod_{i=1} ^ m P_i = \prod_{i=1} ^ m {P(y ^ {(i)}=1 | X ^ {(i)})}^{y ^ {(i)}}{P(y ^ {(i)}=0 | X ^ {(i)})} ^ {1-y ^ {(i)}}以上函数称为似然函数。我们的目标便是最大化似然函数，即找到使联合概率（也就是似然函数）最大的参数$\theta$。为了方便求解，对似然函数取对数，得到对数似然函数： log L(\theta) = \sum_{i=1} ^ m y ^ {(i)}log \frac{1}{1+e ^ {-\theta ^ TX}} + (1-y ^ {(i)})log \frac{1}{1+e ^ {\theta ^ TX}}对对数似然函数取负号，求平均，就得到了LR模型的损失函数： J(\theta) = -\frac{1}{m}\sum_{i=1}^m (y ^ {(i)}log \frac{1}{1+e ^ {-\theta^TX}} + (1-y^{(i)})log \frac{1}{1+e ^ {\theta^TX}})现在我们的目标就就很明确了：优化$J(\theta)$，找出使$J(\theta)$最小的参数$\theta$，就是我们认为的最优的$\theta$。 3. 梯度下降法梯度下降法是常用的优化算法之一。针对LR，梯度下降法的过程如下：首先，有 J(\theta) = -\frac{1}{m}\sum_{i=1}^m (y ^ {(i)}log h(x^{(i)}) + (1-y^{(i)})log (1-h(x^{(i)}))其中$h(x) = \frac{1}{1+e ^ {-\theta^TX}}$接下来求损失函数$J(\theta)$对参数$\theta$的梯度： \frac{\partial J(\theta)}{\partial \theta} = - \frac{1}{m}\sum_{i=1}^m[\frac{y ^ {(i)}}{h(x^{(i)})}·\frac{\partial h(x^{(i)})}{\partial \theta} - \frac{1- y ^ {(i)}}{1 - h(x^{(i)})}·\frac{\partial h(x^{(i)})}{\partial \theta}] = -\frac{1}{m} \sum_{i=1}^m\frac{\partial h(x^{(i)})}{\partial \theta}·\frac{y^{(i)} - h(x^{(i)})}{ h(x^{(i)})(1 - h(x^{(i)}))}将h(x)带入上式： \frac{\partial J(\theta)}{\partial \theta} = \frac{1}{m}\sum_{i=1}^m( h(x^{(i)}) - y^{(i)})·x ^ {(i)}得到迭代公式： \theta := \theta - \alpha\frac{1}{m}\sum_{i=1}^m( h(x^{(i)}) - y^{(i)})·x ^ {(i)}$\alpha$和$\frac{1}{m}$为常数，因此将其合并，得到： \theta := \theta - \alpha\sum_{i=1}^m( h(x^{(i)}) - y^{(i)})·x ^ {(i)}$\alpha$称为learning rate。注意，公式中的变量均为向量形式。至此，我们得到了参数$\theta$的迭代公式，不断迭代直到收敛或损失函数变化很小，我们就得到了最优参数$\theta$ 4. 正则化4.1 假设空间与奥卡姆剃刀原理给定我们一组数据，我们认为这组数据为观测结果。符合这组观测结果的假设有很多。我们可以把学习过程看做在所有假设组成的空间中进行搜索的过程。在这个搜索过程中，我们可能会找到许多满足这组观测数据的假设。比如，坐标系下两点，我们可以用直线拟合，也可以用二次曲线拟合等等。当出现两个模型均很好地符合当前数据时，我们有一个选择模型的指导原则： 奥卡姆剃刀原理：这是一种常用的、自然科学研究中最基本的原则，即“若有多个假设与观察一致，则选最简单的那个” 因此，我们需要在模型的学习过程中，加入这个指导原则。这个过程，就叫正则化。又叫结构风险最小化。 4.2 L0/L1/L2正则化一般，我们认为，参数越少，模型越简单。理想情况下，我们可以在损失函数中加入（不为0的）参数个数来惩罚模型复杂度。即 J(\theta) = -\frac{1}{m}\sum_{i=1}^m (y ^ {(i)}log h(x^{(i)}) + (1-y^{(i)})log (1-h(x^{(i)})) + \lambda\sum_{i=1}^V1（\theta_i）其中，V为参数数量，并且 \begin{equation} 1(x) = \begin{cases} 0, & \text{if x = 0}\newline 1, & \text{if x != 0} \end{cases} \end{equation}这种方法叫做L0正则化。但实际中并不会使用L0正则化，因为很难求解。因此实际中往往使用L1或L2正则化来替代L0正则化。L1正则化为： \lambda||\theta||_1 = \lambda \sum _{i=1}^V|\theta|L2正则化为： \lambda||\theta||_2 = \lambda \sqrt{\sum _ {i=1}^V \theta_i ^ 2}L1正则化和L2正则化都对参数的大小进行了惩罚。这里先说明一个问题： 为什么认为参数越小模型复杂度也越小呢？因为越复杂的模型，越是尝试对所有样本进行拟合，包括一些异常点。这会导致模型在较小的输入区间内，产生较大的输出波动。较大的波动代表着这个区间内导数大，而只有较大的参数才会产生较大的导数。因此参数越大，我们认为模型越复杂。 在L1和L2正则化的选择中，我们需要知道：L1正则化会产生更加稀疏的解，即求得的参数中会有更多的0；L2正则化会产生更多非0但值较小的参数。 也就是说，L1正则化会过滤掉一些无用特征（参数为0，特征就不起作用了），因此L1正则化也是一种特征选择方法。只不过与我们平时手动选择特征不同的是，L1正则化是一种嵌入式地特征选择方法，其特征选择过程与模型的训练过程融为一体，同时完成了。 所以，当所有特征中只有少部分起作用，而我们人工无法辨别时，可以用L1正则化。当大部分特征都能起作用时，使用L2正则化也许更合适。 5. 性能度量模型训练完毕后，我们需要指标来评价其性能。在回归任务中，常用的性能度量是“均方误差”： \frac{1}{m}\sum_{i=1}^m(h(x ^ {(i)}) - y^{(i)}) ^ 2二分类常用的性能度量为查准率、查全率与F1。对于二分类问题，可将样本根据其真实类别与预测类别的组合划分为如下情况： 真实情况 预测情况 正例 反例 正例 TP(真正例) FN(假反例) 反例 FP(假正例) TN(真反例) 查准率P与查全率分别定义为： P = \frac{TP}{TP+FP}R = \frac{TP}{TP+FN}直观上理解，查准率就是我们预测的正例中，有多大比例预测正确了；查全率则是，在原始样本的所有正例中，我们有多大比例预测正确了。 查准率与查全率是一对相互矛盾的指标。一般来说，查准率高时，查全率往往偏低；查全率高时，查准率往往偏低。通常两个模型的查准率与查全率无法比较孰优孰劣时，我们应该综合考虑这两个指标。常用的是F1度量： F1 = \frac{2·P·R}{P+R} = \frac{2·TP}{样例总数+TP-TN}在平时的应用过程中，可以根据实际任务对查准率或查全率的要求来改变权重，来获取更一般的F度量。 写在后面本文从广义线性模型出发，推导了LR模型的产生，损失函数的建立，如何去优化损失函数，正则化以及性能度量。希望能够帮助到大家。由于所学粗浅，文中如有错误或不足，还请各位读者批评指正，感激不尽！我的邮箱：3120104930@zju.edu.cn]]></content>
      <tags>
        <tag>ML</tag>
      </tags>
  </entry>
</search>
