<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[写给20届毕业生的求职指南]]></title>
    <url>%2F2019%2F01%2F14%2F%E6%B1%82%E8%81%8C%E6%8C%87%E5%8D%97%2F</url>
    <content type="text"><![CDATA[0.写在前面记得去年的这个时候，我还在为求职感到迷茫。这份迷茫，一部分原因是对自己能力和定位认知的不足，还有一部分原因是对即将到来的春招和秋招所知甚少。为此，也咨询了一些师兄师姐，提前获取了一些资讯。如今，经过2018年春招、实习和秋招的洗礼，再回头看，一些信息和注意点已经变得显而易见。但我相信20届毕业生，和去年的我一样，对于即将到来的求职季充满了迷茫。 因此，我写这篇文章，主要是梳理一下整个求职季的环节、时间节点和注意事项等。希望能帮助20届毕业生提前窥视求职季全貌，从而在整个求职时间线上把控求职进度。 1. 春招其实春招的面向人群是19届毕业生和20届毕业生。对于前者是招募正式员工，对于后者则是招募具备转正资格的暑期实习生。 1.1 时间节点其实现在互联网行业的招聘越来越提前，比如vivo一月份就开启了春招、六月份就开启了秋招。但普遍来说，大规模的春招集中在三四月份，并一直持续到六月份。从时间上来说，建议2月底左右就要开始关注春招信息和准备投递简历了。如果前期结果不理想，也不要放弃，春招会持续到六月份，机会依然很多。 1.2 信息渠道关于综合性渠道：前几年听师兄师姐介绍求职经验的时候，说的比较多的求职渠道还是应届生网和大街网这种求职网站，不过我身边用这些的好像很少。大家用的比较多的还是一些求职类公众号，毕竟每天用微信，公众号接收信息还是比较方便的。这里推荐一个公号【校招薪水】，是我一个学长运营的，推荐关注。对了，求职季有一些公众号会推出一些转发拉群的活动，建议别浪费时间转发了，没啥用。 关于单一渠道：就是各个公司的官网和招聘公众号了，比如【腾讯招聘】、【拼多多招聘】公众号之类的。这些渠道的优点是信息推送非常及时，能让你第一时间知晓各公司开启招聘渠道的时间节点。 关于抱团：就是求职者们聚在一起共享信息，互帮互助，建议大家都找到自己的小团体哦。无团可抱的非科班程序猿本猿只得自己拉群了 1.3 关于内推先说一下内推的流程吧，候选人找到目标公司的内推人-一般是公司正式员工或实习生，内推人将候选人的简历提交到公司简历池当中，各个部门各个组长会去简历池中筛选简历，相中某个简历后，组长将该简历锁定，然后发起面试；面试通过进入offer流程，不通过则解锁简历，可以继续被捞起面试。 大部分公司都有内推渠道，但各个公司对待内推简历的态度不同。比如阿里，基本上就是上述流程，内推可以免去笔试，直达面试（当然首先要有人相中你的简历）。再比如字节跳动，内推也只是面去简历筛选而已，依然要参加笔试。 内推最大的好处就是获得了一枚复活币，增加了一次投递机会。也就是说，内推失败依然可以参加正式批招聘。不过有些公司如阿里腾讯，对于每一次面试都会有记录，一次失败记录可能会影响正式批面试官的评价，所以，一定要谨慎对待内推，做好充分准备再去面试。 关于内推人，尽可能选择比较熟的师兄师姐，避免选择网上公开发布的内推信息。后者一般是内部员工为了内推奖励而发布的信息，内推量巨大，很可能就遗漏了个别人的内推，而你又无法联系上他们，会白白浪费很多时间（我的血泪教训）。 1.4 关于暑期实习offer选择最好的当然是能去自己目标城市的目标公司，然后最好的结果是顺利转正留下！但是！相信很多人和我一样…并没有想好目标城市或者目标公司。那这个时候，个人建议选择越大的公司越好（其他因素相差不是很大的情况下）。毕竟暑期实习的最终目标是正式工作，即使不能留任，一份大公司的实习是非常为简历增色的，能够有效帮助到自己的秋招。 2. 实习和秋招2.1 暑期实习这里每个人去的公司和状态都不同，我只想说一点，就是实习秋招要兼顾。平时完成实习任务的额外时间，也要给自己留条后路，同时为秋招做准备。 针对暑期实习，企业的目的是为了考察候选人，而我们的目的则是通过暑期实习留用或者为自己的简历增色从而找到更好的工作。无论你的目的是前者还是后者，都建议在尽量争取转正的同时，进行秋招的“广撒网”。注意，我这里的情况只是针对广大人民群众，不包括很牛的人（这些人也不需要看我这篇拙文…..）。 如果你的目的是实习留用，并且成功地留了下来，我的看法是，依然继续参加秋招。原因有如下几点： 放弃秋招意味着放弃了更多机会。可能你觉得当前这个offer已经很满意了，但其实可能还有更好的机会可供选择。多试试，不要给自己留遗憾。 准备秋招也是巩固基础，快速进步的一段时间。毕竟，有目标，有动力，又有面试官帮助你复习和答疑，这种机会真的不多。 和不同公司不同职级的面试官交流，能够让你更加了解自己所在的行业以及自己所从事的方向在行业中的应用场景。虽然小编是做技术的，但我一直认为对整个行业的动向还是要有了解的。往大了说，这其实就是一个人的行业格局问题。在自己面临选择时，这种格局能够帮助自己做出更加正确的选择。 如果你的目的是找到更好的工作，那就更不用说了，多去试吧！ 2.2 时间节点都说“金9银10”，好像现在已经过时了。秋招最早在六月份就已经开始了！（是的，又是那个vivo） 七月中旬到八月底，会有一大批提前批招聘开启。注意，提前批很重要！很重要！之前和同学讨论，有许多人后悔提前批没有多投一些，导致正式批的时候hc（head count，坑位）大大减少，许多hc都被提前批的同学给占了。所以，建议一定要重视提前批，争取在提前批能够拿下一些重量级offer，这样秋招会轻松很多。 2.3 信息渠道和内推见1.2和1.3 2.4 薪资谈判写到这里，悲催的发现我好像从来没有谈判过薪资。突然有点后悔(〒︿〒)。 不过没吃过猪肉，还是见过很多猪跑的。所谓薪资谈判，就是比如你有阿里和腾讯的offer，可以拿着阿里的offer去和腾讯说我要加薪（当然过程可能没这么简单粗暴）。 建议谈薪适度即可。见过一个骚操作，有人拿阿里offer和腾讯谈薪，然后又拿腾讯谈完的薪资去和阿里谈薪.（类似于左脚踩右脚，右脚踩左脚然后想上天？）…..不知道那位仁兄怎样了。 还见过一个公司，就不提名字了，为了挽留候选人-我们群里的一位同学，慢慢一步步加薪，最后相比原offer加了6k（月薪）。虽然最后薪资非常可观，但我们都认为这家公司去不得。后面进行大幅度加薪，说明原本给的薪资是压价非常严重的，而且是一步步慢慢加薪，说明这家公司格局太小，去不得。 2.5 offer选择正式offer选择不同于实习offer选择，要考虑的因素太多。可以参考我之前的offer选择文章。]]></content>
      <tags>
        <tag>求职</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深度学习服务器完整配置指南]]></title>
    <url>%2F2018%2F05%2F20%2F%E6%9C%8D%E5%8A%A1%E5%99%A8%E9%85%8D%E7%BD%AE%E6%8C%87%E5%8D%97%2F</url>
    <content type="text"><![CDATA[最近实验室配了一台深度学习服务器，服务器刚到时基本上只装了 ubuntu 操作系统，所以我也基本上是算从头配置了一台深度学习服务器。现将配置过程记录于此，以便帮助他人和方便交流。 首先，我的深度学习服务器的配置为： ubuntu 14.04 操作系统，16.04 应该也是一样的 Anaconda3 集成的 python 环境，使用 conda 进行虚拟环境管理 jupyterhub 提供的 jupyter notebook 功能 cuda 9.0 + cudnn 7.1 + tensorflow 1.5.0 有了以上配置或环境，服务器就非常好用了。无论是用户虚拟环境管理，还是使用 jupyter notebook 都非常方便。 好了，接下来开始逐个介绍。 1. 网络配置服务器刚到，第一步就是要进行网络配置。网络配置好之后，就可以将服务器放入机房，不用再忍受噪音，进行远程操作了。我这里的服务器是接入的校园网，因此也只介绍校园网下的网络配置，其他情况的同学需要去查一下资料配置。 1.1 接入校园网首先，使用网线介入校园网，打通物理层面。注意不要接错网口，我之前因为接错网口，导致花了半下午时间找问题，可以说是很尴尬了。 接下来，使用 ifconfig 命令来查看网卡类型，eth0 or em1 这种，我这里是 em1。然后修改 /etc/network/interfaces 如下：12345678910111213# This file describes the network interfaces available on your system# and how to activate them. For more information, see interfaces(5).# The loopback network interfaceauto loiface lo inet loopback# The primary network interfaceauto em1#iface em1 inet dhcpiface em1 inet staticaddress xx.xx.xx.xxnetmask xxx.xxx.xxx.0gateway xx.xx.xx.xxxdns-nameservers xx.xx.x.xx 上面的配置其实就是给服务器设置一个静态 ip ，我们可以通过这个静态 ip 访问该服务器。然后配置网管，DNS 服务器什么的。和 windows 下配置类似，实验室用过有线的应该都懂。 配置好之后，就可以连接内网了。使用 ping 10.xx.xxx.xxx（你的校园网内网 ip 地址） 来检查是否能连内网。 注意，我这里记录的是我们学校的校园网接入方式，我不清楚不同学校的校园网接入方式是否相同，需要大家搞清楚。 1.2 镜像源配置1.2.1 更改 apt-get 镜像源将服务器只放在校园网环境下，这样做有两个好处： 安全 校园网速度非常快 但很明显，如果只连校园网，我们将无法下载日常使用的软件或者包。但还好，这个问题很好解决，只需要修改镜像源即可。 我们在使用 apt-get install 命令时，会向外网发送请求，下载我们需要的软件。而修改镜像源之后，则会转而向我们指定的网站发送请求。这里我们使用的是清华的镜像源，清华的镜像源使用校园网内网即可访问，非常适合我们这种校园网的服务器。 首先，把 /etc/apt/source.list 文件备份，然后将 source.list 的内容修改如下（具体内容和 linux 发行版本有关，具体见 Ubuntu 镜像使用帮助）： 123456789101112# 默认注释了源码镜像以提高 apt update 速度，如有需要可自行取消注释deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ trusty main restricted universe multiverse# deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ trusty main restricted universe multiversedeb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ trusty-updates main restricted universe multiverse# deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ trusty-updates main restricted universe multiversedeb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ trusty-backports main restricted universe multiverse# deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ trusty-backports main restricted universe multiversedeb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ trusty-security main restricted universe multiverse# deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ trusty-security main restricted universe multiverse# 预发布软件源，不建议启用# deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ trusty-proposed main restricted universe multiverse# deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ trusty-proposed main restricted universe multiverse 然后就可以在内网下使用 apt-get install 命令了。 1.2.2 更改 pip 和 conda 源使用 pip 或者 conda 安装 python 模块非常方便，但是我们只接入了校园网，所以需要更改一下 pip 和 anaconda 的源。 pip 更改方式如下：修改 ~/.pip/pip.conf（没有就创建一个），内容如下：12[global]index-url = https://pypi.tuna.tsinghua.edu.cn/simple conda 更改方式如下：123# 命令行输入如下命令conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/conda config --set show_channel_urls yes 修改之后，可以测试一下速度，我这里能上 10M/s，还是非常快的。 1.2.3 外网访问如果是在需要访问外网，可以使用 vpn 连接到外网。我这里使用的是校内流传的一个软件，不同学校不同。 使用 sudo dpkg -i xl2tpd_1.2.5%2Bzju-1_amd64.deb 安装之后，按照下面方式配置：123456789101112# 配置用户名和密码$ sudo vpn-connect -cConfigure L2TP VPN for ZJU.# 这里的Username是你的学号，后面的@代表你是10/30/50包月。Username: 216....@10Password: 你的密码# 配置完成，使用这个命令开启连接$ sudo vpn-connect#连接成功，可以ping一下百度，看是否能连外网了$ ping www.baidu.com# 重连，断开等，可以使用下面的命令查看$ vpn-connect -h 以上，基本上就是网络配置的内容了。以上配置好后，基本上就够一个服务器使用了，毕竟，作为计算型服务器，平时使用最多的也就是使用 ssh 远程访问和使用 pip 了。 2. 环境配置这一节主要介绍下 python 环境配置。由于服务器需要多用户使用，因此虚拟环境必不可少。每个用户都使用自身的 python 虚拟环境，可以避免模块版本冲突等问题。此外，jupyter notebook 作为一个方便的 python 交互式环境，能在客户端使用它并且支持多用户使用，也是需要进行配置的。 2.1 python 虚拟环境python 环境我们使用 Anaconda 进行配置。Anaconda 集成了 python 环境和常用的模块，可以帮我们省很多功夫。 使用 sudo apt-get install anaconda3 安装 Anaconda3。也可以去网上下载 anaconda3 的安装包，上传到服务器之后进行安装。Anyway，只要能够成功安装就行，这一步很简单。 之后，可以在命令行输入python 和 conda 来查看是否安装成功。 最后需要修改全局变量，因为此时，只有 root 用户能够使用该环境，其他用户无法正常使用。修改方式如下：12echo 'PATH=/usr/lib/anaconda/bin:$PATH' &gt;&gt; /etc/profile.d/anaconda.shsource /etc/profile 这样，所有用户都可以使用 Anaconda 了，也可以使用 conda 建立虚拟环境了。 建议在使用服务器进行开发时，每位用户都在自己的 python 虚拟环境下进行开发，不要在全局安装 python 的包和库。建立虚拟环境的方式如下：123456list all env: conda info --envcreate a simple virtual env: conda create -n MyEnvName python=3*delete virtual env: conda env remove -n MyEnvName 建立完成后，使用 source activate MyEnvName 进入虚拟环境，使用 source deactivate 推出虚拟环境。 需要注意的是，在自己的虚拟环境下，使用 pip 安装 python 模块，还是安装到全局中的，需要使用 conda -n myEnv install pip 或者在虚拟环境下使用 conda install pip 来安装虚拟环境中的pip，这样在虚拟环境中使用pip就能将包安装到虚拟环境中了。 2.2 jupyter notebook 配置在服务器开启 jupyter notebook 服务，可以在客户端使用浏览器登陆服务器的 jupyter notebook，但是如果多用户同时使用，就需要每个用户都进行配置然后开启服务，非常麻烦。 Jupyterhub 是一组进程，使用 JupyterHub 可以为组中的每个人分别提供单个用户的 Jupyter Notebook 服务器。具体使用方式见JupyterHub。 3. 深度学习环境：cuda9.0 + cudnn7.1 + tensorflow 1.5.0我安装深度学习环境主要是为了使用 tensorflow, 但不同版本的 tensorflow 需要安装不同版本的 cuda ，所以需要先明确自己使用的 tensorflow 版本。我要使用 tensorflow 1.5.0 ，所以选择安装 cuda9.0 + cudnn7.1。 3.1 什么是 cuda 和 cudnnCUDA 是由 NVIDIA 推出的一种集成技术，用于 GPU 的并行计算。 CUDA 的作用，是与通用程序对接。比如，我们使用 python 写的程序，将数据和运算逻辑准备好之后，需要调用 CUDA 库提供的函数来传递给 CUDA，CUDA 再调用显卡驱动对 CUDA 程序进行编译，然后再将编译好的程序和数据传送给 GPU 进行运算。 而 cuDNN 是用于深度神经网络的 GPU 加速计算库。它可以将卷积神经网路的计算变换为对 GPU 更友好的矩阵运算，可以有效提高整个网络的训练速度。 要使用 tensorflow 的 GPU 版本，我们需要安装 CUDA 和 cuDNN。 3.2 安装 cuda9.0 + cudnn7.13.2.1 安装前 首先，确保你的电脑上有 CUDA 支持的 GPU 硬件。 在终端下，输入 lspci | grep -i nvidia 来查看。如果有 GPU 安装，会显示结果。 确保系统中安装了 gcc 使用 gcc --version 来查看 gcc 版本，确保系统中已经安装了 gcc 。 验证系统是否安装了正确的内核头文件和开发包 使用 uname -r 来查看系统的核版本。使用 $ sudo apt-get install linux-headers-$(uname -r) 来安装当前正在运行的内核的内核头文件和开发包。 下载 CUDA CUDA 可以在 NVIDIA 网站 下载，选择自己相应的选项后，就可以下载了。我这里下载的是 CUDA9.0 的 runfile(local) 。 然后上传到服务器。 3.2.2 安装 cuda到之前我们下载的 runfile 文件目录下，运行如下命令：1$ sudo sh cuda_&lt;version&gt;_linux.run 进行安装。 安装程序会提示如下内容： EULA Acceptance：接受协议即可 CUDA Driver installation：如果你已经装好了 nvidia 显卡驱动，这里需要选择 n 。注意，你的 nvidia 显卡驱动版本需要适配 CUDA 版本 CUDA Toolkit installation，location，and /usr/local/cuda symbolic link：这里全选 y 和默认即可 CUDA Samples installation and location：这个最好安装一下，后续可以用于验证 CUDA 是否安装正确 安装完成后，重启系统。 3.2.3 安装后重启系统后，需要修改环境变量。具体操作如下：123$ export PATH=/usr/local/cuda-9.0/bin$&#123;PATH:+:$&#123;PATH&#125;&#125;$ export LD_LIBRARY_PATH=/usr/local/cuda-9.0/lib64\ $&#123;LD_LIBRARY_PATH:+:$&#123;LD_LIBRARY_PATH&#125;&#125; 至此，CUDA 安装完毕。接下来我们验证一下 CUDA 是否安装成功。 检查 /dev/ 目录下是否存在以 nvidia* 开头的多个文件 检查 CUDA Toolkit 是否安装成功 终端输入 nvcc -V 会输出 CUDA 版本信息 编译 Samples 例子 进入到 Samples 安装目录，终端输入 make 进行编译 编译完成后测试 进入 bin/x86_64/linux/release/ 目录。 运行 deviceQuery 程序： $ sudo ./deviceQuery ，查看输出结果，最后一行显示 Result = PASS 表示通过测试。 运行 bandwidthTest 程序： $ sudo ./bandwidthTest, 查看输出结果，最后一行显示 Result = PASS 表示通过测试。 如果以上均没有问题，则说明 CUDA 安装成功。 3.2.4 安装 cudnncudnn 的安装非常简单，只有以下几步： 下载 cudnn 安装包 前往 NVIDIA cuDNN home page, 注册账号之后，下载 cudnn 压缩包（tgz格式）。注意，win10 操作系统在下载 tgz 格式文件时，会将后缀名转变为 .solita* ，这个文件在 linux 下也能解压，但是会报错，实践证明，解压出的文件也是不能用的，因此，建议在 linux 系统或 mac os 下下载上述压缩包。然后上传到服务器。 解压压缩包 输入命令：$ tar -xzvf cudnn-9.0-linux-x64-v7.tgz，会在当前目录下生成一个 cuda/ 目录。 copy 以下文件到 CUDA Toolkit 目录下 终端输入以下命令： 1234$ sudo cp cuda/include/cudnn.h /usr/local/cuda/include$ sudo cp cuda/lib64/libcudnn* /usr/local/cuda/lib64$ sudo chmod a+r /usr/local/cuda/include/cudnn.h$ sudo chmod a+r /usr/local/cuda/lib64/libcudnn* 经过以上三步，cudnn 也安装好了，接下来只需要安装 tensorflow 了！ 3.3 安装 tensorflow最简单的方式当然是使用 pip 进行安装了！ 在终端下输入 pip install tensorflow-gpu==1.5.0 即可。 注意我们安装的是 gpu 版本，所以一定要是 tensorflow-gpu，等号后面代表我们想要安装的 tensorflow 版本。 安装成功后，终端输入 python 进入 python 环境，运行以下代码，若运行成功，则说明我们的 tensorflow 环境也就配置成功了！ 1&gt;&gt;&gt; import tensorflow 4. 总结要配置及维护好一个深度学习服务器还有很长的路要走。比如 ACL 文件权限管理系统，用户权限管理，shell script 学习等。 我这里记录的内容，能够让服务器实现多用户深度学习开发，能对于现在的实验室已经够用了，之后后面的内容，如果有需要会继续学习。 以上内容，最耗费时间的就是 CUDA 和 cudnn 环境的配置了。刚开始偷懒，想跟着中文博客去安装，后来看了数十篇博客，中间碰到各种问题又不断 Google ，解决了一个问题又出现新的问题，真的非常心累。 后来跟着 NVIDIA 的官网教程，重新安装了一遍，总算是成功了。 这让我明白，无论是学习新东西也好，使用一个新工具也好，最好的资料一定是官方编写的。网上铺天盖地的博客，无非是对论文或者官方教程的二次加工。所以一定不要偷懒，耐着性子去琢磨英文版的官方教程，一定是没有问题的！ 我这篇博客其实是对我配置深度学习服务器的梳理，也是对查找的各种资料的总结和对官方教程的二次加工。前面的内容应该很好配置。 如果 CUDA 或 cudnn 的安装出了问题，建议仔细研读 NVIDIA 的官方教程。 Installation Guide Linux :: CUDA Toolkit Documentation cuDNN Installation Guide :: Deep Learning SDK Documentation]]></content>
      <tags>
        <tag>ML</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python 文本编码]]></title>
    <url>%2F2018%2F05%2F13%2Fpython%E6%96%87%E6%9C%AC%E7%BC%96%E7%A0%81%2F</url>
    <content type="text"><![CDATA[转载请注明出处：Python文本编码 相信大家都碰到过令人头疼的python编码问题，比如：&#39;ascii&#39; codec can&#39;t decode byte UnicodeDecodeError: &#39;utf-8&#39; codec can&#39;t decode byte 0xe9 in position 2892: invalid continuation byte 等很多类似的问题。由于博主平时使用python处理自然语言比较多，因此少不了与字符串及其编码打交道。每次碰到这种问题，都要去google个半天，然后按网上的解决方案一个一个试，直到解决为止。 终于有一天，我再也无法忍受这种重复的操作，下定决心一定要把这些问题背后的原因和解决方法搞清楚，于是就有了这片文章。力求搞懂python编码的原理，并总结了常见的问题和解决方案，方便以后查阅。 1. 编码基础1.1 什么是编码和解码首先，我们需要明白，计算机只能处理二进制数字，如果要处理文本，需要把文本转换为二进制数字，然后才能进行处理。 同样，对于文本，计算机存储的是二进制数字，要想得到文本，需要将二进制数字转化为文本，才能呈现出我们看到的内容。 也就是说，我们看到的是文本，计算机处理的是二进制数字。这中间肯定是需要来回转换的。从文本到二进制数字就是编码，从二进制数字到文本就是解码。 1.2 以 ASCII 编码为例以我们最熟悉的 ASCII 编码为例，大写字母 A 的编码是 65，小写字母 a 的编码是 97。1234&gt;&gt;&gt; list('A'.encode('ascii'))[65]&gt;&gt;&gt; list('a'.encode('ascii'))[97] 上面的代码意思是将 A 和 a 以 ASCII 编码方式编码后，得到的数字，这里以十进制表示之，换成二进制就是 ‘A’ -&gt; 01000001‘a’ -&gt; 01100001 就是说，其实我们看到的上述两个字母，在计算机中，是以上面的二进制形式存储的。当我们需要使用或将字母展示出来的时候，计算机需要将二进制数字转化为字母，这个过程就是解码。 ASCII 编码是计算机发展早期发明的，当时只将 127 个字符进行了编码（字符到二进制数字的映射），也就是大小写英文字母，数字和一些符号。ASCII 码采用 8 位 bit 进行编码，也就是一个字节，能表示的最大整数就是 255。当处理中文时，255个字符，显然就不够了。 1.3 中文编码要处理中文，一个字节显然是不够的（中文字符数量可远远不止 255 个）。所以，中国制定了 GB2312 编码，使用两个字节来编码中文。两个字节能够表示 65535 个整数，基本上能够囊括所有的汉字了。让我们来看一下：1234&gt;&gt;&gt; list('中'.encode('gb2312'))[214,208]&gt;&gt;&gt; list('a'.encode('gb2312'))[97] 可以看到，GB2312 编码方式不仅可以将中文进行编码，同时兼容 ASCII 码，使用两者对 ASCII 的 127 个字符进行编码得到的结果是相同的。即 ‘中’ -&gt; 11010110 11010000‘a’ -&gt; 00000000 01100001 我们常见的 gbk 编码方式，其实就是在 gb2312 编码的基础上，增加了一些中文字符。 现在，使用 GB2312 编码方式，我们能处理中英文混杂的文本了。那么问题来了，如果一段文本既有中文，又有英文，还有其他语言呢？ 1.4 大统一：unicode 编码为每种语言制定一套编码方式实在是太蠢了！为什么不能把所有语言的所有字符一起编码呢？ 把所有语言统一到一套编码里，这套编码就是 unicode 编码。使用 unicode 编码，无论处理什么文本都不会出现乱码问题了。 unicode 编码使用两个字节（16 位 bit）表示一个字符，比较偏僻的字符需要使用 4 个字节。 但是新的问题又来了，如果一段纯英文文本，用 unicode 编码存储会比用 ASCII 编码多占用一倍空间！无论是存储还是传输都很浪费！ 1.5 节约小能手：utf-8 编码为了改进上述问题，又提出了 utf-8 编码。该编码将一个 unicode 字符编码成 1~6 个字节，常用的英文字母被编码成 1 个字节，汉字通常是 3 个字节，只有很生僻的字符才会被编码成 4~6 个字节。注意，从 unicode 到 utf-8 并不是直接的对应，而是通过一些算法和规则来转换的。 来看一下具体编码例子吧：1234&gt;&gt;&gt; list('中'.encode('utf-8'))[228, 184, 173]&gt;&gt;&gt; list('a'.encode('utf-8'))[97] 可以看出，utf-8 将汉字 ‘中’ 编码成了三个字节，将英文字母 ‘a’ 编码成了一个字节，且 utf-8 编码兼容 ASCII 编码。 2. Python编码2.1 Python2 or Python3 ?从上面的知识，我们可以知道，字符的最佳定义应当是 Unicode 字符（存储及传输的时候再转为 utf-8）。 从 Python3 的 str 对象中获取的元素是 Unicode 字符，这相当于从 Python2 的 unicode 对象中获取的元素。而 Python2 的 str 对象获取的是原始字节序列（相信用过 Python2 的都见过 ‘\xe8\x32\xa6\xb2……’ 这种乱七八糟的字符吧）。 所以，我们的结论是： 人生苦短，我用Python3 2.2 一个例子让我们来结合Python实例来具体看一下编码的应用：12345678910111213# Python3 字符串，为 Unicode 字符&gt;&gt;&gt; a = '中文'&gt;&gt;&gt; import sys# 查看当前系统默认编码方式（linux）， windows 默认为utf-8&gt;&gt;&gt; sys.getfilesystemencoding()'utf-8'&gt;&gt;&gt; with open('test.txt', 'w', encoding = 'utf-8') as f:··· f.write(a)···&gt;&gt;&gt; with open('/home/zhuyuhe/test.txt', 'r'，encoding = 'utf-8') as f:... print(f.readlines())...['中文'] 上面两个 open 发生了什么的，让我们看一下： 对于第一个 open 函数，它的执行流程是这样的：1[str对象获取 unicode 字符] --&gt;|utf-8 编码| --&gt; [字节序列] --&gt;|二进制形式存储| --&gt; [test.txt] 对于第二个 open 函数，它的执行流程是这样的：1[文件中存放的0101...] --&gt;|utf-8 解码| --&gt; [unicode 字符] --&gt; [str 对象] 这里可能会有一点疑问，为什么 unicode 字符可以通过 utf-8 编码成二进制数字，二进制数字通过 utf-8 解码成 unicode 字符。 前面已经说过，utf-8 是在 unicode 的基础上改进而来，是针对传输和存储而设计的一种编码方式。 utf-8 编码针对的对象是 unicode 字符。 也就是说，在计算机内存中，统一使用 unicode 编码，当需要保存到硬盘或者需要传输的时候，就转换为 utf-8 编码。 2.3 另一个例子如果我们使用记事本打开上一小节保存的 test.txt 呢，这中间发生了什么呢？ 让我们同样用流程图来看一下： 1[在计算机硬盘以0101...形式存放的文件] --&gt; |记事本所使用的编码方式|--&gt; B[我们看到的字符] 我们的 test.txt 是以 utf-8 的编码方式保存的，如果记事本使用的编码方式为 gb2312，打开该文件时，记事本会尝试以 gb2312 的方式解码该文件的字节序列（二进制数字），由于两者的字符与字节对应关系不同，当然会解码失败。此时记事本会显示各种乱码。这个过程类似下面的代码： 1234&gt;&gt;&gt; '中文'.encode('utf-8').decode('gbk')Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt;UnicodeDecodeError: 'gbk' codec can't decode byte 0xad in position 2: illegal multibyte sequence 如果碰到上述问题，可以尝试使用 notepad++ 打开该文件，尝试改变编辑器的编码方式，来尝试是否能正常打开文件。 是的，只能去尝试。因为在没有任何信息的情况下，给我们一串字节序列，我们是不知道它的编码方式的。 所以，在平时的文件存储和传输中，统一编码方式是很重要的。 2.4 统一编码在日常的使用中，避免乱码的重要方式就是统一你的编码方式。 一般我们将编码方式统一为 utf-8，下面给出了一些参考的建议： linux 使用 Python 时，可以使用如下代码查看默认编码： 12&gt;&gt;&gt; import sys&gt;&gt;&gt; sys.getfilesystemencoding() 查看是否为 utf-8，如果不是，改成 utf-8。 windows 默认编码方式为 utf-8，不需要修改 在使用 open 函数及其他读写函数时，加上 encoding=utf-8，确保文件以 utf-8 方式编码和解码。 使用远程连接软件时，比如 XShell 或 MobaXterm ，将软件的编码选为 utf-8，这样可以保证远程连接显示正常。 不要依赖系统的默认编码，打开文件时应始终明确传入 encoding = 参数，因为不同设备使用的默认编码不同，即使是同一设备，也可能会发生变化。 其实，理解了为何需要做上述处理，也基本就理解了编码。碰到编码问题，结合错误信息，基本上就能很快找出发生错误的原因了。不过为了方便起见，我还是将常见的问题总结了下。 3. 常见问题原因及解决方案3.1 UnicodeEncodeError多数非 UTF 编码器只能处理 Unicode 字符的一小部分子集。把文本转换为字节序列时，如果目标编码中没有定义某个字符，就会抛出 UnicodeEncodeError 异常。 看个例子：1234&gt;&gt;&gt; '中文'.encode('ascii')Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt;UnicodeEncodeError: 'ascii' codec can't encode characters in position 0-1: ordinal not in range(128) 当我们尝试将中文字符以 ASCII 方式编码时，报出了 UnicodeEncodeError。这个原因很明显，ASCII 编码方式并没有定义中文字符，无法对中文字符进行编码。 碰到这种问题，可以使用 errors 参数进行处理：12&gt;&gt;&gt; '中文'.encode('ascii', errors='ignore')b'' 我们将错误的编码忽略掉，最后得到空的字节序列。通常，这样做是非常不妥的。 errors 参数还有许多可选项，大家可以自行探索。 比较妥当的解决方案，当然是选择合适的编码方式。这个需要根据具体情况而定，相信，理解了这个错误发生的原因，解决起来就很轻松啦！ 3.2 UnicodeDecodeError不是每一个字节都包含有有效的 ASCII 字符，也不是每一个字符序列都是有效的 utf-8 。因此把二进制序列转换为文本时，遇到无法转换的字节序列就会抛出 UnicodeDecodeError。 看个例子：12345&gt;&gt;&gt; b = b'\xe4\xb8\xad\xe6\x96\x87'&gt;&gt;&gt; b.decode('ascii')Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt;UnicodeDecodeError: 'ascii' codec can't decode byte 0xe4 in position 0: ordinal not in range(128) b 是字符 ‘中文’ 以 utf-8 方式编码形成的字节序列，属于 Python3 中的 bytes 类型。将该字节序列以 ASCII 方式解码时抛出了 UnicodeDecodeError。 4 总结本文详细阐述了编码的原理及各种问题发生的原因，相信搞懂这些，以后碰到编码问题就再也不用求助 Google 了。知道了问题发生的原因，我们自己就能迅速解决了！ 参考文献廖雪峰的官方网站-字符串和编码大道至简：史上最易懂的『乱码』解决方案《流畅的Python》]]></content>
      <tags>
        <tag>编程语言 编码</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习算法系列（1）- 逻辑斯蒂回归]]></title>
    <url>%2F2018%2F01%2F21%2F%E6%B5%85%E8%B0%88%E9%80%BB%E8%BE%91%E6%96%AF%E8%92%82%E5%9B%9E%E5%BD%92%2F</url>
    <content type="text"><![CDATA[转载请注明出处：https://zhuyuhe.github.io/2018/01/21/%E6%B5%85%E8%B0%88%E9%80%BB%E8%BE%91%E6%96%AF%E8%92%82%E5%9B%9E%E5%BD%92/ 博主即将开始求职之旅，于是搭了这个博客，将备战春招和秋招的复习笔记记录于此。此系列是对机器学习算法理论的复习，力求搞懂算法的来龙去脉，公式繁多，废话较少。以后有时间会在此基础上进行完善，力求通俗易懂。如有不足之处，还请各位读者指正！ 1.从广义线性模型到sigmoid函数首先，LR是二分类模型，我们假设二分类问题服从伯努利分布。即其概率分布为： P(y;\phi)= \phi^y (1-\phi)^{1-y} = exp(ylog\phi + (1-y)log(1-\phi))=exp(ylog\frac{\phi}{1-\phi} + log(1-\phi))而伯努利分布属于指数蔟分布，即其概率分布可以写成如下形式： P(y;\eta)= b(y)exp(\eta^TT(y)-\alpha(\eta))其中，$\eta$为自然参数。广义线性模型认为输入x与自然参数$\eta$为线性关系，即$\eta = \theta^TX$。对比上述两式可以得到： b(y) = 1T(y) = y\eta = log\frac{\phi}{1-\phi}所以有 \theta^TX = \eta = log\frac{\phi}{1-\phi}得到 \phi = \frac{1}{1+e ^ {-\theta^TX}}这里的$\phi$就是伯努利分布中的P(Y=1|X)而LR中的模型输出 h(x) = E(y|x) = 0 · P(Y=0|X) + 1 · P(Y=1|X) = \frac{1}{1+e ^ {-\theta^TX}}这就推导出了逻辑斯蒂回归模型。也解释了为什么LR要用sigmoid函数，因为我们从广义线性模型出发，推导出的LR模型刚好就是sigmoid函数形式的。并且sigmoid有很多良好的数学性质： 连续可导 值域为（0,1）,给了模型可解释性，即将输出结果解释为对应于该分类的概率 下一节我们将从推导出的lr模型出发，去看看它的损失函数是怎么来的。 2. 从极大似然估计到损失函数给定一组数据，我们需要用这组数据去找到最好的参数$\theta$。我们认为使观测结果（即现有的数据）出现的概率最大的参数$\theta$就是最优的参数。这种思想就是极大似然估计。假设这组数据独立同分布，其联合概率可以写成各样本出现概率的乘积。即 L(\theta) = \prod_{i=1} ^ m P_i = \prod_{i=1} ^ m {P(y ^ {(i)}=1 | X ^ {(i)})}^{y ^ {(i)}}{P(y ^ {(i)}=0 | X ^ {(i)})} ^ {1-y ^ {(i)}}以上函数称为似然函数。我们的目标便是最大化似然函数，即找到使联合概率（也就是似然函数）最大的参数$\theta$。为了方便求解，对似然函数取对数，得到对数似然函数： log L(\theta) = \sum_{i=1} ^ m y ^ {(i)}log \frac{1}{1+e ^ {-\theta ^ TX}} + (1-y ^ {(i)})log \frac{1}{1+e ^ {\theta ^ TX}}对对数似然函数取负号，求平均，就得到了LR模型的损失函数： J(\theta) = -\frac{1}{m}\sum_{i=1}^m (y ^ {(i)}log \frac{1}{1+e ^ {-\theta^TX}} + (1-y^{(i)})log \frac{1}{1+e ^ {\theta^TX}})现在我们的目标就就很明确了：优化$J(\theta)$，找出使$J(\theta)$最小的参数$\theta$，就是我们认为的最优的$\theta$。 3. 梯度下降法梯度下降法是常用的优化算法之一。针对LR，梯度下降法的过程如下：首先，有 J(\theta) = -\frac{1}{m}\sum_{i=1}^m (y ^ {(i)}log h(x^{(i)}) + (1-y^{(i)})log (1-h(x^{(i)}))其中$h(x) = \frac{1}{1+e ^ {-\theta^TX}}$接下来求损失函数$J(\theta)$对参数$\theta$的梯度： \frac{\partial J(\theta)}{\partial \theta} = - \frac{1}{m}\sum_{i=1}^m[\frac{y ^ {(i)}}{h(x^{(i)})}·\frac{\partial h(x^{(i)})}{\partial \theta} - \frac{1- y ^ {(i)}}{1 - h(x^{(i)})}·\frac{\partial h(x^{(i)})}{\partial \theta}] = -\frac{1}{m} \sum_{i=1}^m\frac{\partial h(x^{(i)})}{\partial \theta}·\frac{y^{(i)} - h(x^{(i)})}{ h(x^{(i)})(1 - h(x^{(i)}))}将h(x)带入上式： \frac{\partial J(\theta)}{\partial \theta} = \frac{1}{m}\sum_{i=1}^m( h(x^{(i)}) - y^{(i)})·x ^ {(i)}得到迭代公式： \theta := \theta - \alpha\frac{1}{m}\sum_{i=1}^m( h(x^{(i)}) - y^{(i)})·x ^ {(i)}$\alpha$和$\frac{1}{m}$为常数，因此将其合并，得到： \theta := \theta - \alpha\sum_{i=1}^m( h(x^{(i)}) - y^{(i)})·x ^ {(i)}$\alpha$称为learning rate。注意，公式中的变量均为向量形式。至此，我们得到了参数$\theta$的迭代公式，不断迭代直到收敛或损失函数变化很小，我们就得到了最优参数$\theta$ 4. 正则化4.1 假设空间与奥卡姆剃刀原理给定我们一组数据，我们认为这组数据为观测结果。符合这组观测结果的假设有很多。我们可以把学习过程看做在所有假设组成的空间中进行搜索的过程。在这个搜索过程中，我们可能会找到许多满足这组观测数据的假设。比如，坐标系下两点，我们可以用直线拟合，也可以用二次曲线拟合等等。当出现两个模型均很好地符合当前数据时，我们有一个选择模型的指导原则： 奥卡姆剃刀原理：这是一种常用的、自然科学研究中最基本的原则，即“若有多个假设与观察一致，则选最简单的那个” 因此，我们需要在模型的学习过程中，加入这个指导原则。这个过程，就叫正则化。又叫结构风险最小化。 4.2 L0/L1/L2正则化一般，我们认为，参数越少，模型越简单。理想情况下，我们可以在损失函数中加入（不为0的）参数个数来惩罚模型复杂度。即 J(\theta) = -\frac{1}{m}\sum_{i=1}^m (y ^ {(i)}log h(x^{(i)}) + (1-y^{(i)})log (1-h(x^{(i)})) + \lambda\sum_{i=1}^V1（\theta_i）其中，V为参数数量，并且 \begin{equation} 1(x) = \begin{cases} 0, & \text{if x = 0}\newline 1, & \text{if x != 0} \end{cases} \end{equation}这种方法叫做L0正则化。但实际中并不会使用L0正则化，因为很难求解。因此实际中往往使用L1或L2正则化来替代L0正则化。L1正则化为： \lambda||\theta||_1 = \lambda \sum _{i=1}^V|\theta|L2正则化为： \lambda||\theta||_2 = \lambda \sqrt{\sum _ {i=1}^V \theta_i ^ 2}L1正则化和L2正则化都对参数的大小进行了惩罚。这里先说明一个问题： 为什么认为参数越小模型复杂度也越小呢？因为越复杂的模型，越是尝试对所有样本进行拟合，包括一些异常点。这会导致模型在较小的输入区间内，产生较大的输出波动。较大的波动代表着这个区间内导数大，而只有较大的参数才会产生较大的导数。因此参数越大，我们认为模型越复杂。 在L1和L2正则化的选择中，我们需要知道：L1正则化会产生更加稀疏的解，即求得的参数中会有更多的0；L2正则化会产生更多非0但值较小的参数。 也就是说，L1正则化会过滤掉一些无用特征（参数为0，特征就不起作用了），因此L1正则化也是一种特征选择方法。只不过与我们平时手动选择特征不同的是，L1正则化是一种嵌入式地特征选择方法，其特征选择过程与模型的训练过程融为一体，同时完成了。 所以，当所有特征中只有少部分起作用，而我们人工无法辨别时，可以用L1正则化。当大部分特征都能起作用时，使用L2正则化也许更合适。 5. 性能度量模型训练完毕后，我们需要指标来评价其性能。在回归任务中，常用的性能度量是“均方误差”： \frac{1}{m}\sum_{i=1}^m(h(x ^ {(i)}) - y^{(i)}) ^ 2二分类常用的性能度量为查准率、查全率与F1。对于二分类问题，可将样本根据其真实类别与预测类别的组合划分为如下情况： 真实情况 预测情况 正例 反例 正例 TP(真正例) FN(假反例) 反例 FP(假正例) TN(真反例) 查准率P与查全率分别定义为： P = \frac{TP}{TP+FP}R = \frac{TP}{TP+FN}直观上理解，查准率就是我们预测的正例中，有多大比例预测正确了；查全率则是，在原始样本的所有正例中，我们有多大比例预测正确了。 查准率与查全率是一对相互矛盾的指标。一般来说，查准率高时，查全率往往偏低；查全率高时，查准率往往偏低。通常两个模型的查准率与查全率无法比较孰优孰劣时，我们应该综合考虑这两个指标。常用的是F1度量： F1 = \frac{2·P·R}{P+R} = \frac{2·TP}{样例总数+TP-TN}在平时的应用过程中，可以根据实际任务对查准率或查全率的要求来改变权重，来获取更一般的F度量。 写在后面本文从广义线性模型出发，推导了LR模型的产生，损失函数的建立，如何去优化损失函数，正则化以及性能度量。希望能够帮助到大家。由于所学粗浅，文中如有错误或不足，还请各位读者批评指正，感激不尽！我的邮箱：3120104930@zju.edu.cn]]></content>
      <tags>
        <tag>ML</tag>
      </tags>
  </entry>
</search>
