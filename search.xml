<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[BERT发展史（二）语言模型]]></title>
    <url>%2F2019%2F01%2F22%2FBERT2%2F</url>
    <content type="text"><![CDATA[0. 写在前面上一篇文章中主要介绍了基于统计方法的词嵌入，这些方法都是使用很长的向量来表示一个词语，且词语的“含义”分布在高维度向量的一个或少数几个分量上（稀疏）。这些方法的主要问题在于用于表示词语的向量维度过高且非常稀疏，并且无法很好的表征词语的含义。那么一个理想的词向量应该是什么样呢？ 想象一下，我们身处于一个充满词语的空间，这个空间中，相似的词语们组成一个“家族”抱团取暖，它们的距离比较近；不相似的词语身处不同的“家族”，距离较远。如“我”“你”“他”“吴彦祖”这些词的在词空间的距离比较近，“香蕉”和“电脑”在词空间的距离比较远。那么词嵌入其实就是某种将词空间映射到向量空间的方法。我们希望这些词语映射到一个固定长度的稠密低维度（相对于词表大小而言）向量，且依然保持以上特性，可以使用向量之间的距离来度量词语之间的相似性，这就是Distributed Representation。这里谈一下“Distributed”这个词的含义。基于统计的词嵌入方法产生的向量非常稀疏，词语的含义集中分布在向量的一个或少数几个非零分量上。而“Distributed”其中文意思是“分布式”，意思是指将词语的语义均匀分布到向量的各个分量上， 每个非零分量都承担其一部分含义。 这篇文章的主角虽然是语言模型，但词向量与神经语言模型相伴相生，Word2Vec其实就是在神经语言模型的基础上改进发展而来的。所以理解好语言模型对理解Word2Vec也有很大帮助。 1. 语言模型相信大家都用过或者听过小米智能印象或者天猫精灵，这类产品都用到了语音识别技术，即，将语音识别为自然语言文字。某天，LN不分的算法小白菜同学对着小爱同学说： 老(nao)奶奶(lailai)喝牛(liu)奶(lai) 语音识别技术将这句话转为文字有以下几种可能： 老来来喝牛奶 老奶奶喝刘奶 老奶奶喝牛奶 …… 问题来了，这么多可能，选择哪个作为最终结果呢？当然是最合理的，或者说概率最高的。这几个句子中，3号句子在现实中存在的概率是最高的，所以我们选择3号作为最终识别的结果。而语言模型，就是用来计算一个句子出现概率的模型。对于上例来说，就是分别计算P(老，来来，喝，牛奶)、P(老，奶奶，喝，刘奶)和P(老，奶奶，喝，牛奶)，取概率最大的句子作为结果。 2. 统计语言模型我们可以将句子看做单词序列：$w_1、w_2……w_n$ 。那么语言模型的目标就是计算 P(w_1,w_2......w_n)=P(w_1)P(w_2|w_1)......P(w_n|w_1,w_2...w_{n-1})​ps. 以上公式可以通过条件概率和链式法则推得，也将句子的生成视作句子填充事件的链条直接得到。 对于上面的例子，就是计算P(老，奶奶，喝，牛奶) = P(老)*P(奶奶 | 老)*P(喝|老,奶奶)*P(牛奶|老,奶奶,喝)。 由此可以看出，求一个句子出现概率的核心是求各个词语出现的条件概率。 在统计语言模型中，该条件概率通过极大似然估计计算： P(w_n|w_1,w_2...w_{n-1})=\frac{count(w_1,w_2...w_n)}{count(w_1,w_2......w_n-1)}​但以上方法存在两个问题： 要求的概率太多了，因为w1,w2….wn的组合实在太多了（n的词表数次方）； 由于语料的数量有限，数据中可能不存在w1,w2,….wn的组合，导致求得的条件概率为0。 针对第一个问题，可以引入马尔科夫假设。这个假设很简单，就是假设任意一个词出现的概率只与它前面出现的有限n个词有关。引入马尔科夫假设的语言模型又叫n-gram语言模型。我们假设n=1，可以得到： P(w_1,w_2......w_n)=P(w_1)P(w_2|w_1)P(w_3|w_2)......P(w_n|w_{n-1})条件概率同样通过极大似然估计计算： P(w_n|w_{n-1})=\frac{count(w_{n-1},w_n)}{count(w_n-1)}这样计算量就少了很多。 针对第二个问题，引入平滑技术。平滑技术的思想和目的是将数据集中看到的概率分配一点给未出现的数据（避免概率为0），并且保持总的概率和为1。这里就不细讲了。 这里讨论一下马尔科夫假设的合理性。为什么可以这样假设？首先这种假设的有效性肯定是经过实践验证的，这里想讨论的是直觉上的合理性。思考一下在写“老奶奶喝牛奶”这句话时，到写“牛奶”这个词的时候，我们更多关注的是“喝”这个词，我们思考（下意识）更多的是与“喝”这个词匹配的词，而非与“老奶奶喝”匹配的词。因此，引入马尔科夫假设虽然会损失一些信息，但确实能够进行对句子的概率进行近似，并且可以极大地方便计算。 上面提出了两个问题，这里从机器学习的角度理解一下其本质。将条件概率看做语言模型中的参数，问题一在于语言模型的参数太多了；问题二在于由于数据量有限，导致许多参数无法估计。即：相对数据量而言，模型太复杂了，也就是说模型过拟合了！一个简单的例子就是数据集中没出现的句子，该语言模型会将这个句子出现的概率计算为0（未引入n-gram和平滑之前）。即模型的泛化能力不够，无法处理没“见过”的句子。 针对问题一引入马尔科夫假设，就是降低模型复杂度；针对问题二引入平滑，则类似于引入了正则化，对参数进行了约束。这些都是机器学习中解决模型过拟合，提高模型泛化能力的方法。 求出所有的条件概率之后，针对一个新的句子，只需要将对应的条件概率连乘即可得到句子的概率了。 最后，继续思考一个问题：句子1：“A dog is running in the room”和句子2：”A cat is running in the room”两个句子哪个概率大一些？直觉上来说应该是相似的，但统计语言模型非常受数据集的影响，无法考虑词语与词语的相似度，所以以上两个句子的概率计算结果可能是相差很大的。比如数据集中句子1出现了100次，句子2只出现了1次，那最后的计算结果P(句子1)将远大于P(句子2)。下面介绍的前向神经网络语言模型能比较好地解决这个问题。 3. 神经网络语言模型前面说过，语言模型的核心是求词语出现的条件概率。说起条件概率，其实机器学习中许多分类模型就是对给定输入x求结果y的条件概率进行建模的。那么，是否可以使用机器学习中的分类模型来对语言模型进行建模呢？机器学习中，一般是对问题构造一个目标函数，然后使用数据对目标函数进行优化，求得一组最优参数，然后使用这组参数对应的模型来进行预测。对于语言模型来说，可以将目标函数设置为： \prod _ { w \in \mathcal { C } } p ( w | \operatorname { Context } ( w ) )其中C表示语料（Corpus），Contex(w)表示词w的上下文。由此可见，我们可以使用机器学习训练得到词语w 的上下文到词语w这个类别的映射关系。2003年，Bengio等人发表的《A Neural Probabilistic Language Model》论文中，使用前向神经网络表征了这种映射关系。 在介绍模型细节之前，我们先来捋一捋。我们需要训练的是一个全连接神经网络模型，输入是词语w前面的n个词语（上下文），输出是预测为所有词的概率，我们希望词语w对应的概率最大化。问题来了，词语w的上下文怎么输入神经网络？论文中的做法是使用随机初始化的方法建立一个|V|×k大小的查找表(lookup table)，该表内的每一行向量唯一表示一个词语。 以“老 奶奶 喝 牛奶”作为语料库为例，我们可以得到一个大小|V|=4的词表：{“老”，“奶奶”，“喝”，“牛奶”}。为每个词编号，得到：{0: “老”, 1: “奶奶”, 2: “喝”, 3: “牛奶”}。假设每个词向量的维度为3，随机初始化一个4×3大小的矩阵（查找表）如下(这里是自己编的数字，实际是随机生成的服从一定分布的数字)： \left[ \begin{matrix} 0.1 & 0.2 & 0.3 \\ 0.4 & 0.5 & 0.6 \\ 0.7 & 0.8 & 0.9 \\ 0.2 & 0.4 & 0.8 \\ \end{matrix} \right]\tag{1}其中，[0.1,0.2,0.3]表示编号为0的词语“老”的词向量，以此类推。 以“老”、“奶奶”、“喝”预测“牛奶”为例（上下文长度为3），模型的结构图如下： 训练时，输入词去查找表中找到对应的词向量，然后将输入词的词向量拼接在一起，形成一个长度为3×k（k为词向量维度）的输入向量。经过一层隐层后，输出层使用softmax函数将输出映射到(0,1)中，输出层的数字表示输出为对应编号词语的概率。这个例子中，我们希望最大化输出层最后一个数字，代表P(牛奶|老，奶奶，牛奶)。注意，这个样本的训练过程只用到了上下文词语：“老”，“奶奶”，“喝”的词向量，没有用到“牛奶”的词向量。此外，需要注意的是，与通常的神经网络的输入都是已知的不同，我们之前初始化的矩阵查找表是和神经网络的参数同时训练更新的！ 以上是一个样本的训练过程，如果我们的句子是“老 奶奶 喝 牛奶 和 咖啡”，我们还可以构造出另外的样本：((奶奶,喝,牛奶), 和)，((喝,牛奶,和), 咖啡)。 通过这个方法训练的语言模型有什么优势呢？ 首先，相似词的词向量也是相似的（下篇文章讲Word2Vec的时候会证明），在句子中替换相似词对结果的影响很小（输入相似，模型参数一定）。A cat is running in the room和A dog is running in the room可以获得相似的句子概率。 然后，预测结果肯定不会为0，自备平滑功能。 现在想想，我们最初的目标是训练一个神经网络语言模型，模型训练完成后，我们可以得到矩阵查找表（也就是词向量）和神经网络的模型参数。也就是说我们得到了两个产物。前者-词向量是本次模型训练过程中的副产物！而这些词向量具备我们之前说的特性：低维度稠密向量，可以通过距离度量词语相似度。这一点很重要，后面的Word2Vec正是来源于这个思想，这点下篇文章再说。 本小节的最后，还是来思考一下，当前这个神经网络语言模型有什么缺点吧： 模型输入为固定数量的上下文词语，无法获取更远词语的信息； 词表的大小一般都比较大，神经网络的输出层softmax函数的计算量会非常大，模型训练效率很低。 下一节和下篇文章介绍的模型将会解决这些缺点。 4. 循环神经网络语言模型前面说过，神经网络语言模型由于使用的是全连接神经网络，其输入是定长的，所使用的上下文词语数量需要在模型训练前确定。因此在预测当前词时，该模型只能使用一定长度上下文的信息。为了解决该问题，Mikolov在2010年发表了论文《Recurrent Neural Network Based Language Model》，这篇论文中使用RNN替换全连接神经网络对语言模型进行建模。 这个模型的结构如下： 这个模型的思想就是通过使用RNN上一个时间步的隐层信息，代替距离当前词距离大于1的上下文词语的信息。 前面讲过，n-gram为了简化模型和方便计算，认为当前词语出现的概率只与前面有限几个词有关。而RNN语言模型则可以做到使用前面所有词的信息，又不使用海量的参数。但需要注意的是，RNN训练过程中存在梯度爆炸和消失问题，可能并不能获取过长距离的词语信息。此外，上一节中介绍的神经网络语言模型，其上下文使用的词向量具备一定的含义，而RNN使用上一时间步的隐层向量，其含义不甚明了。 5. 参考资料 A Neural Probabilistic Language Model Recurrent Neural Network Based Language Model Word2Vec中的数学原理详解 知乎：深入浅出讲解语言模型 系列文章： BERT发展史（一）从词嵌入讲起]]></content>
      <tags>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[BERT发展史（一）从词嵌入讲起]]></title>
    <url>%2F2019%2F01%2F18%2FBERT1%2F</url>
    <content type="text"><![CDATA[0. 写在前面要说NLP最近最火的是什么，那必是BERT无疑了。强悍如BERT，在十多项NLP的任务上强势屠榜，业界声称BERT将开创NLP领域新的纪元。在BERT刚出来的时候，就在各种公众号上看过各种原理解释，但一直没有去认真研究论文。后来在PaperWeekly公众号上阅读了万字长文《NLP的巨人肩膀》，算是把BERT的诞生史过了一遍。在看这篇文章的过程中发现Word2Vec到BERT中间的那么多演变自己都没有好好研究过，于是打算写BERT发展史系列文章，从传统的词嵌入开始，详细讲解词嵌入往BERT发展的过程及其中的思想的演变过程。这一系列涉及到很多算法，基本上涵盖了大部分比较重要的NLP技术和思想，有些是我接触过的，有些我还没有接触过，因此也希望写作的过程能够push自己去学习，去进步。 我希望在这一系列文章中尽量以自己的话语阐述各个算法背后的思想和原理，多输出一些自己的理解，并在最后附上我个人认为学习这些算法比较好的材料和教程。如果看我的文章没有太理解，也可以多去看看参考材料。 1.机器学习流程什么是机器学习？抛开严肃的定义，在我看来，机器学习（这里特指有监督学习）就是使用计算机对事件的结果进行建模的过程。用数学符号来表示就是$f(x)-&gt;y​$，其中$y​$代表事件的结果，$x​$代表影响事件结果的各个因素，f代表我们使用的机器学习算法。即，我们希望将事件结果与影响结果的因素以某种函数的形式映射起来。 比如，主人公算法小白菜现在在上小学五年级，我们要预测他在期末考试中能否及格。事件的结果就是能及格和不能及格，影响事件结果的因素则有很多，这些因素有的对预测结果很重要，有的则无关紧要。比如算法小白菜在每次月考中的成绩，对于预测就很重要，而算法小白菜的身高则无关紧要。机器学习中，挖掘影响事件结果因素的过程称为特征工程，这一步的目的是依据现有数据挖掘出对结果有影响的因素（也叫特征），然后使用这些特征对结果进行建模。 因此，一个大致的数据挖掘流程为：数据预处理—&gt;特征工程—&gt;建模。很多时候，挖掘和发现出好的特征对最后结果的影响甚至要大于模型本身。 其实说了这么多，我只是想说明，好的特征对于建模的重要性。就像我们这世界有果皆有因，一件事情发生的背后，定有各种因素在背后支撑。我们希望计算机能够帮助我们建立各种因素取值和事件结果的对应关系，但在此之前，如何确定这些因素则是个让人头疼的东西。 2. 词嵌入在互联网中，我们每天都会接触到海量的文本信息。所谓NLP，就是使用计算机处理自然语言的过程。而我们都知道，计算机只能处理数值，因此自然语言需要以一定的形式转化为数值。词嵌入就是将词语（word）映射为数字的方式。一个单纯的实数包含的信息太少，一般我们映射为一个数值向量。这时我们会发现一个问题，怎么把词语转换为数值向量？自然语言本身蕴含了语义和句法等特征，如何在转换过程中保留这些抽象的特征？这点其实很重要，因为如果没有将自然语言的特征很好的保留下来，后续的所有工作就是对一些无意义的信息进行建模，自然得不到好的结果。纵观NLP的发展史，很多革命性的成果都是词嵌入的发展成果，如Word2Vec、ELMo和BERT，其实都是很好地将自然语言的特征在转换过程中进行了保留。 接下来我会以例子的形式介绍几个基于频率的词嵌入方法。 2.1 词频向量比如我们有两个文档（Document）： D1：我 叫 算法 小 白菜 。我 很 懒惰 。 D2：算法 小 白菜 是 一个 懒惰 的 人 。 这里已经对句子进行了分词，即将各个句子拆分为词语的组合，词语之间用空格隔开。 这两篇文档包含的所有非重复词构成一个词典（dictionary）：{“我”，“叫”，“算法”，“小”，“白菜”，“。”，“很”，“懒惰”，“是”，“一个”，“的”，“人”}。 这样每篇文档中词典中词语出现的频率如下表所示： 我 叫 算法 小 白菜 。 很 懒惰 是 一个 的 人 D1 2 1 1 1 1 2 1 1 0 0 0 0 D2 0 0 1 1 1 1 0 1 1 1 1 1 D1行“我”列的数值为2代表在文档D1中，“我”这个词出现了两次。现在我们通过以上表格得到了一个词频矩阵。我们可以使用列向量来表示一个词语，比如使用[2,0]来表示“我”这个词；使用行向量来表示一篇文档，比如使用[2,1,1,1,1,2,1,1,0,0,0,0]来表示文档D1。 OK，以上就是使用词频向量进行词嵌入的过程。但在实际使用过程中许多细节会有变种。比如词典的组织方式。在真实世界中，我们的语料数据通常是包含数十万到上百万篇文档的，这些文档包含的非重复词会非常多。而每篇文档包含的词语数量相对于词典中的词语数量是非常小的，这会导致词频矩阵中会出现非常多的0。所以组织词典的另外一个方式是按词语出现的频率排序，选择频率最高的前1000个词语组成词典。 2.2 TF-IDF向量首先思考下词频矩阵有什么局限性。比如一篇关于游戏的文章，游戏这个词的词频会比较高，但像“的”、“和”这种通用词（NLP中叫停用词）出现的频率也非常高。从特征的角度出发，游戏这个词对这篇文章来说是个重要特征，“的”、“和”这种词则是不重要的特征，但词频矩阵这种词嵌入方式的结果却给予了这两种特征相似的重要性。 TF-IDF（词频-逆文档频率）可以通过给予通用词较小的计算权重来解决这个问题。继续看例子。 我们有两篇文档D1和D2，每篇文章中所包含的词语及其词频如下图所示： TF的定义如下： TF = (词语w在一篇文档中出现的次数)/（文档中的词语总数） 根据这个定义，可以计算TF(关于，D1) = 1/9；TF(学习，D2) = 3/9。 TF表示的是该词语对于该文档的贡献程度，我们认为与文章相关的词语出现的频率应该会比较高。 IDF的定义如下： IDF = log（（文档的数量）/ （包含词语w的文档数量）） 根据这个定义，得到IDF(关于) = log(2/2) = 0；IDF(游戏) = log(2/1) = 0.301。 IDF想表达的是，如果一个词语在大部分甚至所有文档中都出现（词语中的中央空调），那么这个词语对于任意一篇文档都是不重要的。 最后，TF-IDF的计算方式就是把TF和IDF相乘即可。来看一下： TF-IDF(关于，D1) = （1/9）* 0 = 0 TF-IDF(游戏，D1) = （3/9） * 0.301 = 0.1003 TF-IDF(学习，D2) = （3/9）* 0.301 = 0.1003 这下就比较符合我们的直觉了：重要的词语（特征）应该具有更高的权重。 将词频矩阵中的词频值替换为TF-IDF值，我们就可以得到文档和词语的向量表示了。同时，由于TF-IDF的特征，TF-IDF还经常被用来提取文章中的关键词。 2.3 词共现向量这个方法的出发点是：相似的词语会经常同时出现，并且具有相似的上下文。比如：苹果是水果。香蕉是水果。苹果和香蕉具有相似的上下文。 在深入词共现矩阵的构造和定义之前，我们先来介绍两个概念：共现和上下文窗口。 共现 — 对于给定的语料，共现指词语w1和词语w2在给定上下文窗口中共同出现的次数。 上下文窗口 — 计算共现时所指定的窗口大小，由距离和方向指定。 同样以一个例子来说明： Corpus = He is not lazy. He is intelligent. He is smart. 给定距离2和双向的上下文窗口，该例的词共现矩阵如下： 以图中的红色框和蓝色框为例。红色框表示“He”和“is”的共现次数为4，如下图所示： 而蓝色框值为0，代表词“lazy”从未出现在词“intelligent”的方圆两词之内。 在词共现矩阵中，我们可以使用行向量或者列向量来表示词语（矩阵是对称的）。但存在的问题与词频矩阵相似，向量的维度等于词典的大小V，这将导致向量是稀疏且高维的，不利于计算。在实际使用中，通常是使用PCA或SVD等技术，将词共现矩阵降维为V×k（k&lt;&lt;V）的大小，这样，每个词语将使用一个k维大小的向量来表示。 3. 几点想法 自然语言的特征是非常抽象的，如何将“苹果”这个词的语义以数值向量的方式体现出来是非常有挑战性的。词频向量和TF-IDF向量都是从词语和文档的关系角度出发的，从词语对文档的重要程度出发去完成词语/文档到向量的转换。而词共现矩阵则是从词语与词语之间的关系角度出发，设法抽取词语的含义。这给我们一种思路，单纯地考虑如何将某个词的语义映射到向量不太可能实现，那是否可以将词语映射到向量空间之后依然保留词语与词语之间的关系？ 基于统计的词嵌入方法在什么样的任务中可以取得好的结果呢？同样从特征的角度出发，词频向量和TF-IDF向量提取的是词语对文档的重要性特征，什么样的任务需要这种特征呢？比如垃圾邮件识别，一些垃圾邮件一般有经常出现的“垃圾”词语，就可以使用基于统计的词嵌入方法；比如关键词提取，就可以使用TF-IDF。而一些需要复杂语义和句法特征的任务，如机器阅读理解，机器翻译等，就无法使用这些词嵌入方法。 之前被面试官问到过一个问题：基于统计的词嵌入方法和基于预测的词嵌入方法（如Word2Vec）有什么区别，为什么深度学习中要使用后者。个人观点是，一个方面，基于统计的词嵌入方法一旦在数据确定之后，词的向量便就确定，在不同的NLP任务中都要使用相同的词向量，无法根据不同任务进行调整。而基于预测的词嵌入可以在不同的任务和模型中更新词向量，使词向量逐步地适应这项任务；另一方面则是基于统计的词嵌入方法提取的语义信息实在太少，无法取到比较好的结果。 4. 参考资料An Intuitive Understanding of Word Embeddings: From Count Vectors to Word2Vec [DM] 都是套路: 从上帝视角看透时间序列和数据挖掘]]></content>
      <tags>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[写给20届毕业生的求职指南]]></title>
    <url>%2F2019%2F01%2F17%2F%E6%B1%82%E8%81%8C%E6%8C%87%E5%8D%97%2F</url>
    <content type="text"><![CDATA[0.写在前面记得去年的这个时候，我还在为求职感到迷茫。这份迷茫，一部分原因是对自己能力和定位认知的不足，还有一部分原因是对即将到来的春招和秋招所知甚少。为此，也咨询了一些师兄师姐，提前获取了一些资讯。如今，经过2018年春招、实习和秋招的洗礼，再回头看，一些信息和注意点已经变得显而易见。但我相信20届毕业生，和去年的我一样，对于即将到来的求职季充满了迷茫。 因此，我写这篇文章，主要是梳理一下整个求职季的环节、时间节点和注意事项等。希望能帮助20届毕业生提前窥视求职季全貌，从而在整个求职时间线上把控求职进度。 1. 春招其实春招的面向人群是19届毕业生和20届毕业生。对于前者是招募正式员工，对于后者则是招募具备转正资格的暑期实习生。 1.1 时间节点其实现在互联网行业的招聘越来越提前，比如vivo一月份就开启了春招、六月份就开启了秋招。但普遍来说，大规模的春招集中在三四月份，并一直持续到六月份。从时间上来说，建议2月底左右就要开始关注春招信息和准备投递简历了。如果前期结果不理想，也不要放弃，春招会持续到六月份，机会依然很多。 1.2 信息渠道关于综合性渠道：前几年听师兄师姐介绍求职经验的时候，说的比较多的求职渠道还是应届生网和大街网这种求职网站，不过我身边用这些的好像很少。大家用的比较多的还是一些求职类公众号，毕竟每天用微信，公众号接收信息还是比较方便的。这里推荐一个公号【校招薪水】，是我一个学长运营的，推荐关注。对了，求职季有一些公众号会推出一些转发拉群的活动，建议别浪费时间转发了，没啥用。 关于单一渠道：就是各个公司的官网和招聘公众号了，比如【腾讯招聘】、【拼多多招聘】公众号之类的。这些渠道的优点是信息推送非常及时，能让你第一时间知晓各公司开启招聘渠道的时间节点。 关于抱团：就是求职者们聚在一起共享信息，互帮互助，建议大家都找到自己的小团体哦。无团可抱的非科班程序猿本猿只得自己拉群了 1.3 关于内推先说一下内推的流程吧，候选人找到目标公司的内推人-一般是公司正式员工或实习生，内推人将候选人的简历提交到公司简历池当中，各个部门各个组长会去简历池中筛选简历，相中某个简历后，组长将该简历锁定，然后发起面试；面试通过进入offer流程，不通过则解锁简历，可以继续被捞起面试。 大部分公司都有内推渠道，但各个公司对待内推简历的态度不同。比如阿里，基本上就是上述流程，内推可以免去笔试，直达面试（当然首先要有人相中你的简历）。再比如字节跳动，内推也只是面去简历筛选而已，依然要参加笔试。 内推最大的好处就是获得了一枚复活币，增加了一次投递机会。也就是说，内推失败依然可以参加正式批招聘。不过有些公司如阿里腾讯，对于每一次面试都会有记录，一次失败记录可能会影响正式批面试官的评价，所以，一定要谨慎对待内推，做好充分准备再去面试。 关于内推人，尽可能选择比较熟的师兄师姐，避免选择网上公开发布的内推信息。后者一般是内部员工为了内推奖励而发布的信息，内推量巨大，很可能就遗漏了个别人的内推，而你又无法联系上他们，会白白浪费很多时间（我的血泪教训）。 1.4 关于暑期实习offer选择最好的当然是能去自己目标城市的目标公司，然后最好的结果是顺利转正留下！但是！相信很多人和我一样…并没有想好目标城市或者目标公司。那这个时候，个人建议选择越大的公司越好（其他因素相差不是很大的情况下）。毕竟暑期实习的最终目标是正式工作，即使不能留任，一份大公司的实习是非常为简历增色的，能够有效帮助到自己的秋招。 2. 实习和秋招2.1 暑期实习这里每个人去的公司和状态都不同，我只想说一点，就是实习秋招要兼顾。平时完成实习任务的额外时间，也要给自己留条后路，同时为秋招做准备。 针对暑期实习，企业的目的是为了考察候选人，而我们的目的则是通过暑期实习留用或者为自己的简历增色从而找到更好的工作。无论你的目的是前者还是后者，都建议在尽量争取转正的同时，进行秋招的“广撒网”。注意，我这里的情况只是针对广大人民群众，不包括很牛的人（这些人也不需要看我这篇拙文…..）。 如果你的目的是实习留用，并且成功地留了下来，我的看法是，依然继续参加秋招。原因有如下几点： 放弃秋招意味着放弃了更多机会。可能你觉得当前这个offer已经很满意了，但其实可能还有更好的机会可供选择。多试试，不要给自己留遗憾。 准备秋招也是巩固基础，快速进步的一段时间。毕竟，有目标，有动力，又有面试官帮助你复习和答疑，这种机会真的不多。 和不同公司不同职级的面试官交流，能够让你更加了解自己所在的行业以及自己所从事的方向在行业中的应用场景。虽然小编是做技术的，但我一直认为对整个行业的动向还是要有了解的。往大了说，这其实就是一个人的行业格局问题。在自己面临选择时，这种格局能够帮助自己做出更加正确的选择。 如果你的目的是找到更好的工作，那就更不用说了，多去试吧！ 2.2 时间节点都说“金9银10”，好像现在已经过时了。秋招最早在六月份就已经开始了！（是的，又是那个vivo） 七月中旬到八月底，会有一大批提前批招聘开启。注意，提前批很重要！很重要！之前和同学讨论，有许多人后悔提前批没有多投一些，导致正式批的时候hc（head count，坑位）大大减少，许多hc都被提前批的同学给占了。所以，建议一定要重视提前批，争取在提前批能够拿下一些重量级offer，这样秋招会轻松很多。 2.3 信息渠道和内推见1.2和1.3 2.4 薪资谈判写到这里，悲催的发现我好像从来没有谈判过薪资。突然有点后悔(〒︿〒)。 不过没吃过猪肉，还是见过很多猪跑的。所谓薪资谈判，就是比如你有阿里和腾讯的offer，可以拿着阿里的offer去和腾讯说我要加薪（当然过程可能没这么简单粗暴）。 建议谈薪适度即可。见过一个骚操作，有人拿阿里offer和腾讯谈薪，然后又拿腾讯谈完的薪资去和阿里谈薪.（类似于左脚踩右脚，右脚踩左脚然后想上天？）…..不知道那位仁兄怎样了。 还见过一个公司，就不提名字了，为了挽留候选人-我们群里的一位同学，慢慢一步步加薪，最后相比原offer加了6k（月薪）。虽然最后薪资非常可观，但我们都认为这家公司去不得。后面进行大幅度加薪，说明原本给的薪资是压价非常严重的，而且是一步步慢慢加薪，说明这家公司格局太小，去不得。 2.5 offer选择正式offer选择不同于实习offer选择，要考虑的因素太多。可以参考我之前的offer选择文章。]]></content>
      <tags>
        <tag>求职</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深度学习服务器完整配置指南]]></title>
    <url>%2F2019%2F01%2F17%2F%E6%9C%8D%E5%8A%A1%E5%99%A8%E9%85%8D%E7%BD%AE%E6%8C%87%E5%8D%97%2F</url>
    <content type="text"><![CDATA[最近实验室配了一台深度学习服务器，服务器刚到时基本上只装了 ubuntu 操作系统，所以我也基本上是算从头配置了一台深度学习服务器。现将配置过程记录于此，以便帮助他人和方便交流。 首先，我的深度学习服务器的配置为： ubuntu 14.04 操作系统，16.04 应该也是一样的 Anaconda3 集成的 python 环境，使用 conda 进行虚拟环境管理 jupyterhub 提供的 jupyter notebook 功能 cuda 9.0 + cudnn 7.1 + tensorflow 1.5.0 有了以上配置或环境，服务器就非常好用了。无论是用户虚拟环境管理，还是使用 jupyter notebook 都非常方便。 好了，接下来开始逐个介绍。 1. 网络配置服务器刚到，第一步就是要进行网络配置。网络配置好之后，就可以将服务器放入机房，不用再忍受噪音，进行远程操作了。我这里的服务器是接入的校园网，因此也只介绍校园网下的网络配置，其他情况的同学需要去查一下资料配置。 1.1 接入校园网首先，使用网线介入校园网，打通物理层面。注意不要接错网口，我之前因为接错网口，导致花了半下午时间找问题，可以说是很尴尬了。 接下来，使用 ifconfig 命令来查看网卡类型，eth0 or em1 这种，我这里是 em1。然后修改 /etc/network/interfaces 如下：12345678910111213# This file describes the network interfaces available on your system# and how to activate them. For more information, see interfaces(5).# The loopback network interfaceauto loiface lo inet loopback# The primary network interfaceauto em1#iface em1 inet dhcpiface em1 inet staticaddress xx.xx.xx.xxnetmask xxx.xxx.xxx.0gateway xx.xx.xx.xxxdns-nameservers xx.xx.x.xx 上面的配置其实就是给服务器设置一个静态 ip ，我们可以通过这个静态 ip 访问该服务器。然后配置网管，DNS 服务器什么的。和 windows 下配置类似，实验室用过有线的应该都懂。 配置好之后，就可以连接内网了。使用 ping 10.xx.xxx.xxx（你的校园网内网 ip 地址） 来检查是否能连内网。 注意，我这里记录的是我们学校的校园网接入方式，我不清楚不同学校的校园网接入方式是否相同，需要大家搞清楚。 1.2 镜像源配置1.2.1 更改 apt-get 镜像源将服务器只放在校园网环境下，这样做有两个好处： 安全 校园网速度非常快 但很明显，如果只连校园网，我们将无法下载日常使用的软件或者包。但还好，这个问题很好解决，只需要修改镜像源即可。 我们在使用 apt-get install 命令时，会向外网发送请求，下载我们需要的软件。而修改镜像源之后，则会转而向我们指定的网站发送请求。这里我们使用的是清华的镜像源，清华的镜像源使用校园网内网即可访问，非常适合我们这种校园网的服务器。 首先，把 /etc/apt/source.list 文件备份，然后将 source.list 的内容修改如下（具体内容和 linux 发行版本有关，具体见 Ubuntu 镜像使用帮助）： 123456789101112# 默认注释了源码镜像以提高 apt update 速度，如有需要可自行取消注释deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ trusty main restricted universe multiverse# deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ trusty main restricted universe multiversedeb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ trusty-updates main restricted universe multiverse# deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ trusty-updates main restricted universe multiversedeb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ trusty-backports main restricted universe multiverse# deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ trusty-backports main restricted universe multiversedeb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ trusty-security main restricted universe multiverse# deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ trusty-security main restricted universe multiverse# 预发布软件源，不建议启用# deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ trusty-proposed main restricted universe multiverse# deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ trusty-proposed main restricted universe multiverse 然后就可以在内网下使用 apt-get install 命令了。 1.2.2 更改 pip 和 conda 源使用 pip 或者 conda 安装 python 模块非常方便，但是我们只接入了校园网，所以需要更改一下 pip 和 anaconda 的源。 pip 更改方式如下：修改 ~/.pip/pip.conf（没有就创建一个），内容如下：12[global]index-url = https://pypi.tuna.tsinghua.edu.cn/simple conda 更改方式如下：123# 命令行输入如下命令conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/conda config --set show_channel_urls yes 修改之后，可以测试一下速度，我这里能上 10M/s，还是非常快的。 1.2.3 外网访问如果是在需要访问外网，可以使用 vpn 连接到外网。我这里使用的是校内流传的一个软件，不同学校不同。 使用 sudo dpkg -i xl2tpd_1.2.5%2Bzju-1_amd64.deb 安装之后，按照下面方式配置：123456789101112# 配置用户名和密码$ sudo vpn-connect -cConfigure L2TP VPN for ZJU.# 这里的Username是你的学号，后面的@代表你是10/30/50包月。Username: 216....@10Password: 你的密码# 配置完成，使用这个命令开启连接$ sudo vpn-connect#连接成功，可以ping一下百度，看是否能连外网了$ ping www.baidu.com# 重连，断开等，可以使用下面的命令查看$ vpn-connect -h 以上，基本上就是网络配置的内容了。以上配置好后，基本上就够一个服务器使用了，毕竟，作为计算型服务器，平时使用最多的也就是使用 ssh 远程访问和使用 pip 了。 2. 环境配置这一节主要介绍下 python 环境配置。由于服务器需要多用户使用，因此虚拟环境必不可少。每个用户都使用自身的 python 虚拟环境，可以避免模块版本冲突等问题。此外，jupyter notebook 作为一个方便的 python 交互式环境，能在客户端使用它并且支持多用户使用，也是需要进行配置的。 2.1 python 虚拟环境python 环境我们使用 Anaconda 进行配置。Anaconda 集成了 python 环境和常用的模块，可以帮我们省很多功夫。 使用 sudo apt-get install anaconda3 安装 Anaconda3。也可以去网上下载 anaconda3 的安装包，上传到服务器之后进行安装。Anyway，只要能够成功安装就行，这一步很简单。 之后，可以在命令行输入python 和 conda 来查看是否安装成功。 最后需要修改全局变量，因为此时，只有 root 用户能够使用该环境，其他用户无法正常使用。修改方式如下：12echo 'PATH=/usr/lib/anaconda/bin:$PATH' &gt;&gt; /etc/profile.d/anaconda.shsource /etc/profile 这样，所有用户都可以使用 Anaconda 了，也可以使用 conda 建立虚拟环境了。 建议在使用服务器进行开发时，每位用户都在自己的 python 虚拟环境下进行开发，不要在全局安装 python 的包和库。建立虚拟环境的方式如下：123456list all env: conda info --envcreate a simple virtual env: conda create -n MyEnvName python=3*delete virtual env: conda env remove -n MyEnvName 建立完成后，使用 source activate MyEnvName 进入虚拟环境，使用 source deactivate 推出虚拟环境。 需要注意的是，在自己的虚拟环境下，使用 pip 安装 python 模块，还是安装到全局中的，需要使用 conda -n myEnv install pip 或者在虚拟环境下使用 conda install pip 来安装虚拟环境中的pip，这样在虚拟环境中使用pip就能将包安装到虚拟环境中了。 2.2 jupyter notebook 配置在服务器开启 jupyter notebook 服务，可以在客户端使用浏览器登陆服务器的 jupyter notebook，但是如果多用户同时使用，就需要每个用户都进行配置然后开启服务，非常麻烦。 Jupyterhub 是一组进程，使用 JupyterHub 可以为组中的每个人分别提供单个用户的 Jupyter Notebook 服务器。具体使用方式见JupyterHub。 3. 深度学习环境：cuda9.0 + cudnn7.1 + tensorflow 1.5.0我安装深度学习环境主要是为了使用 tensorflow, 但不同版本的 tensorflow 需要安装不同版本的 cuda ，所以需要先明确自己使用的 tensorflow 版本。我要使用 tensorflow 1.5.0 ，所以选择安装 cuda9.0 + cudnn7.1。 3.1 什么是 cuda 和 cudnnCUDA 是由 NVIDIA 推出的一种集成技术，用于 GPU 的并行计算。 CUDA 的作用，是与通用程序对接。比如，我们使用 python 写的程序，将数据和运算逻辑准备好之后，需要调用 CUDA 库提供的函数来传递给 CUDA，CUDA 再调用显卡驱动对 CUDA 程序进行编译，然后再将编译好的程序和数据传送给 GPU 进行运算。 而 cuDNN 是用于深度神经网络的 GPU 加速计算库。它可以将卷积神经网路的计算变换为对 GPU 更友好的矩阵运算，可以有效提高整个网络的训练速度。 要使用 tensorflow 的 GPU 版本，我们需要安装 CUDA 和 cuDNN。 3.2 安装 cuda9.0 + cudnn7.13.2.1 安装前 首先，确保你的电脑上有 CUDA 支持的 GPU 硬件。 在终端下，输入 lspci | grep -i nvidia 来查看。如果有 GPU 安装，会显示结果。 确保系统中安装了 gcc 使用 gcc --version 来查看 gcc 版本，确保系统中已经安装了 gcc 。 验证系统是否安装了正确的内核头文件和开发包 使用 uname -r 来查看系统的核版本。使用 $ sudo apt-get install linux-headers-$(uname -r) 来安装当前正在运行的内核的内核头文件和开发包。 下载 CUDA CUDA 可以在 NVIDIA 网站 下载，选择自己相应的选项后，就可以下载了。我这里下载的是 CUDA9.0 的 runfile(local) 。 然后上传到服务器。 3.2.2 安装 cuda到之前我们下载的 runfile 文件目录下，运行如下命令：1$ sudo sh cuda_&lt;version&gt;_linux.run 进行安装。 安装程序会提示如下内容： EULA Acceptance：接受协议即可 CUDA Driver installation：如果你已经装好了 nvidia 显卡驱动，这里需要选择 n 。注意，你的 nvidia 显卡驱动版本需要适配 CUDA 版本 CUDA Toolkit installation，location，and /usr/local/cuda symbolic link：这里全选 y 和默认即可 CUDA Samples installation and location：这个最好安装一下，后续可以用于验证 CUDA 是否安装正确 安装完成后，重启系统。 3.2.3 安装后重启系统后，需要修改环境变量。具体操作如下：123$ export PATH=/usr/local/cuda-9.0/bin$&#123;PATH:+:$&#123;PATH&#125;&#125;$ export LD_LIBRARY_PATH=/usr/local/cuda-9.0/lib64\ $&#123;LD_LIBRARY_PATH:+:$&#123;LD_LIBRARY_PATH&#125;&#125; 至此，CUDA 安装完毕。接下来我们验证一下 CUDA 是否安装成功。 检查 /dev/ 目录下是否存在以 nvidia* 开头的多个文件 检查 CUDA Toolkit 是否安装成功 终端输入 nvcc -V 会输出 CUDA 版本信息 编译 Samples 例子 进入到 Samples 安装目录，终端输入 make 进行编译 编译完成后测试 进入 bin/x86_64/linux/release/ 目录。 运行 deviceQuery 程序： $ sudo ./deviceQuery ，查看输出结果，最后一行显示 Result = PASS 表示通过测试。 运行 bandwidthTest 程序： $ sudo ./bandwidthTest, 查看输出结果，最后一行显示 Result = PASS 表示通过测试。 如果以上均没有问题，则说明 CUDA 安装成功。 3.2.4 安装 cudnncudnn 的安装非常简单，只有以下几步： 下载 cudnn 安装包 前往 NVIDIA cuDNN home page, 注册账号之后，下载 cudnn 压缩包（tgz格式）。注意，win10 操作系统在下载 tgz 格式文件时，会将后缀名转变为 .solita* ，这个文件在 linux 下也能解压，但是会报错，实践证明，解压出的文件也是不能用的，因此，建议在 linux 系统或 mac os 下下载上述压缩包。然后上传到服务器。 解压压缩包 输入命令：$ tar -xzvf cudnn-9.0-linux-x64-v7.tgz，会在当前目录下生成一个 cuda/ 目录。 copy 以下文件到 CUDA Toolkit 目录下 终端输入以下命令： 1234$ sudo cp cuda/include/cudnn.h /usr/local/cuda/include$ sudo cp cuda/lib64/libcudnn* /usr/local/cuda/lib64$ sudo chmod a+r /usr/local/cuda/include/cudnn.h$ sudo chmod a+r /usr/local/cuda/lib64/libcudnn* 经过以上三步，cudnn 也安装好了，接下来只需要安装 tensorflow 了！ 3.3 安装 tensorflow最简单的方式当然是使用 pip 进行安装了！ 在终端下输入 pip install tensorflow-gpu==1.5.0 即可。 注意我们安装的是 gpu 版本，所以一定要是 tensorflow-gpu，等号后面代表我们想要安装的 tensorflow 版本。 安装成功后，终端输入 python 进入 python 环境，运行以下代码，若运行成功，则说明我们的 tensorflow 环境也就配置成功了！ 1&gt;&gt;&gt; import tensorflow 4. 总结要配置及维护好一个深度学习服务器还有很长的路要走。比如 ACL 文件权限管理系统，用户权限管理，shell script 学习等。 我这里记录的内容，能够让服务器实现多用户深度学习开发，能对于现在的实验室已经够用了，之后后面的内容，如果有需要会继续学习。 以上内容，最耗费时间的就是 CUDA 和 cudnn 环境的配置了。刚开始偷懒，想跟着中文博客去安装，后来看了数十篇博客，中间碰到各种问题又不断 Google ，解决了一个问题又出现新的问题，真的非常心累。 后来跟着 NVIDIA 的官网教程，重新安装了一遍，总算是成功了。 这让我明白，无论是学习新东西也好，使用一个新工具也好，最好的资料一定是官方编写的。网上铺天盖地的博客，无非是对论文或者官方教程的二次加工。所以一定不要偷懒，耐着性子去琢磨英文版的官方教程，一定是没有问题的！ 我这篇博客其实是对我配置深度学习服务器的梳理，也是对查找的各种资料的总结和对官方教程的二次加工。前面的内容应该很好配置。 如果 CUDA 或 cudnn 的安装出了问题，建议仔细研读 NVIDIA 的官方教程。 Installation Guide Linux :: CUDA Toolkit Documentation cuDNN Installation Guide :: Deep Learning SDK Documentation]]></content>
      <tags>
        <tag>ML</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python 文本编码]]></title>
    <url>%2F2019%2F01%2F17%2Fpython%E6%96%87%E6%9C%AC%E7%BC%96%E7%A0%81%2F</url>
    <content type="text"><![CDATA[转载请注明出处：Python文本编码 相信大家都碰到过令人头疼的python编码问题，比如：&#39;ascii&#39; codec can&#39;t decode byte UnicodeDecodeError: &#39;utf-8&#39; codec can&#39;t decode byte 0xe9 in position 2892: invalid continuation byte 等很多类似的问题。由于博主平时使用python处理自然语言比较多，因此少不了与字符串及其编码打交道。每次碰到这种问题，都要去google个半天，然后按网上的解决方案一个一个试，直到解决为止。 终于有一天，我再也无法忍受这种重复的操作，下定决心一定要把这些问题背后的原因和解决方法搞清楚，于是就有了这片文章。力求搞懂python编码的原理，并总结了常见的问题和解决方案，方便以后查阅。 1. 编码基础1.1 什么是编码和解码首先，我们需要明白，计算机只能处理二进制数字，如果要处理文本，需要把文本转换为二进制数字，然后才能进行处理。 同样，对于文本，计算机存储的是二进制数字，要想得到文本，需要将二进制数字转化为文本，才能呈现出我们看到的内容。 也就是说，我们看到的是文本，计算机处理的是二进制数字。这中间肯定是需要来回转换的。从文本到二进制数字就是编码，从二进制数字到文本就是解码。 1.2 以 ASCII 编码为例以我们最熟悉的 ASCII 编码为例，大写字母 A 的编码是 65，小写字母 a 的编码是 97。1234&gt;&gt;&gt; list('A'.encode('ascii'))[65]&gt;&gt;&gt; list('a'.encode('ascii'))[97] 上面的代码意思是将 A 和 a 以 ASCII 编码方式编码后，得到的数字，这里以十进制表示之，换成二进制就是 ‘A’ -&gt; 01000001‘a’ -&gt; 01100001 就是说，其实我们看到的上述两个字母，在计算机中，是以上面的二进制形式存储的。当我们需要使用或将字母展示出来的时候，计算机需要将二进制数字转化为字母，这个过程就是解码。 ASCII 编码是计算机发展早期发明的，当时只将 127 个字符进行了编码（字符到二进制数字的映射），也就是大小写英文字母，数字和一些符号。ASCII 码采用 8 位 bit 进行编码，也就是一个字节，能表示的最大整数就是 255。当处理中文时，255个字符，显然就不够了。 1.3 中文编码要处理中文，一个字节显然是不够的（中文字符数量可远远不止 255 个）。所以，中国制定了 GB2312 编码，使用两个字节来编码中文。两个字节能够表示 65535 个整数，基本上能够囊括所有的汉字了。让我们来看一下：1234&gt;&gt;&gt; list('中'.encode('gb2312'))[214,208]&gt;&gt;&gt; list('a'.encode('gb2312'))[97] 可以看到，GB2312 编码方式不仅可以将中文进行编码，同时兼容 ASCII 码，使用两者对 ASCII 的 127 个字符进行编码得到的结果是相同的。即 ‘中’ -&gt; 11010110 11010000‘a’ -&gt; 00000000 01100001 我们常见的 gbk 编码方式，其实就是在 gb2312 编码的基础上，增加了一些中文字符。 现在，使用 GB2312 编码方式，我们能处理中英文混杂的文本了。那么问题来了，如果一段文本既有中文，又有英文，还有其他语言呢？ 1.4 大统一：unicode 编码为每种语言制定一套编码方式实在是太蠢了！为什么不能把所有语言的所有字符一起编码呢？ 把所有语言统一到一套编码里，这套编码就是 unicode 编码。使用 unicode 编码，无论处理什么文本都不会出现乱码问题了。 unicode 编码使用两个字节（16 位 bit）表示一个字符，比较偏僻的字符需要使用 4 个字节。 但是新的问题又来了，如果一段纯英文文本，用 unicode 编码存储会比用 ASCII 编码多占用一倍空间！无论是存储还是传输都很浪费！ 1.5 节约小能手：utf-8 编码为了改进上述问题，又提出了 utf-8 编码。该编码将一个 unicode 字符编码成 1~6 个字节，常用的英文字母被编码成 1 个字节，汉字通常是 3 个字节，只有很生僻的字符才会被编码成 4~6 个字节。注意，从 unicode 到 utf-8 并不是直接的对应，而是通过一些算法和规则来转换的。 来看一下具体编码例子吧：1234&gt;&gt;&gt; list('中'.encode('utf-8'))[228, 184, 173]&gt;&gt;&gt; list('a'.encode('utf-8'))[97] 可以看出，utf-8 将汉字 ‘中’ 编码成了三个字节，将英文字母 ‘a’ 编码成了一个字节，且 utf-8 编码兼容 ASCII 编码。 2. Python编码2.1 Python2 or Python3 ?从上面的知识，我们可以知道，字符的最佳定义应当是 Unicode 字符（存储及传输的时候再转为 utf-8）。 从 Python3 的 str 对象中获取的元素是 Unicode 字符，这相当于从 Python2 的 unicode 对象中获取的元素。而 Python2 的 str 对象获取的是原始字节序列（相信用过 Python2 的都见过 ‘\xe8\x32\xa6\xb2……’ 这种乱七八糟的字符吧）。 所以，我们的结论是： 人生苦短，我用Python3 2.2 一个例子让我们来结合Python实例来具体看一下编码的应用：12345678910111213# Python3 字符串，为 Unicode 字符&gt;&gt;&gt; a = '中文'&gt;&gt;&gt; import sys# 查看当前系统默认编码方式（linux）， windows 默认为utf-8&gt;&gt;&gt; sys.getfilesystemencoding()'utf-8'&gt;&gt;&gt; with open('test.txt', 'w', encoding = 'utf-8') as f:··· f.write(a)···&gt;&gt;&gt; with open('/home/zhuyuhe/test.txt', 'r'，encoding = 'utf-8') as f:... print(f.readlines())...['中文'] 上面两个 open 发生了什么的，让我们看一下： 对于第一个 open 函数，它的执行流程是这样的：1[str对象获取 unicode 字符] --&gt;|utf-8 编码| --&gt; [字节序列] --&gt;|二进制形式存储| --&gt; [test.txt] 对于第二个 open 函数，它的执行流程是这样的：1[文件中存放的0101...] --&gt;|utf-8 解码| --&gt; [unicode 字符] --&gt; [str 对象] 这里可能会有一点疑问，为什么 unicode 字符可以通过 utf-8 编码成二进制数字，二进制数字通过 utf-8 解码成 unicode 字符。 前面已经说过，utf-8 是在 unicode 的基础上改进而来，是针对传输和存储而设计的一种编码方式。 utf-8 编码针对的对象是 unicode 字符。 也就是说，在计算机内存中，统一使用 unicode 编码，当需要保存到硬盘或者需要传输的时候，就转换为 utf-8 编码。 2.3 另一个例子如果我们使用记事本打开上一小节保存的 test.txt 呢，这中间发生了什么呢？ 让我们同样用流程图来看一下： 1[在计算机硬盘以0101...形式存放的文件] --&gt; |记事本所使用的编码方式|--&gt; B[我们看到的字符] 我们的 test.txt 是以 utf-8 的编码方式保存的，如果记事本使用的编码方式为 gb2312，打开该文件时，记事本会尝试以 gb2312 的方式解码该文件的字节序列（二进制数字），由于两者的字符与字节对应关系不同，当然会解码失败。此时记事本会显示各种乱码。这个过程类似下面的代码： 1234&gt;&gt;&gt; '中文'.encode('utf-8').decode('gbk')Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt;UnicodeDecodeError: 'gbk' codec can't decode byte 0xad in position 2: illegal multibyte sequence 如果碰到上述问题，可以尝试使用 notepad++ 打开该文件，尝试改变编辑器的编码方式，来尝试是否能正常打开文件。 是的，只能去尝试。因为在没有任何信息的情况下，给我们一串字节序列，我们是不知道它的编码方式的。 所以，在平时的文件存储和传输中，统一编码方式是很重要的。 2.4 统一编码在日常的使用中，避免乱码的重要方式就是统一你的编码方式。 一般我们将编码方式统一为 utf-8，下面给出了一些参考的建议： linux 使用 Python 时，可以使用如下代码查看默认编码： 12&gt;&gt;&gt; import sys&gt;&gt;&gt; sys.getfilesystemencoding() 查看是否为 utf-8，如果不是，改成 utf-8。 windows 默认编码方式为 utf-8，不需要修改 在使用 open 函数及其他读写函数时，加上 encoding=utf-8，确保文件以 utf-8 方式编码和解码。 使用远程连接软件时，比如 XShell 或 MobaXterm ，将软件的编码选为 utf-8，这样可以保证远程连接显示正常。 不要依赖系统的默认编码，打开文件时应始终明确传入 encoding = 参数，因为不同设备使用的默认编码不同，即使是同一设备，也可能会发生变化。 其实，理解了为何需要做上述处理，也基本就理解了编码。碰到编码问题，结合错误信息，基本上就能很快找出发生错误的原因了。不过为了方便起见，我还是将常见的问题总结了下。 3. 常见问题原因及解决方案3.1 UnicodeEncodeError多数非 UTF 编码器只能处理 Unicode 字符的一小部分子集。把文本转换为字节序列时，如果目标编码中没有定义某个字符，就会抛出 UnicodeEncodeError 异常。 看个例子：1234&gt;&gt;&gt; '中文'.encode('ascii')Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt;UnicodeEncodeError: 'ascii' codec can't encode characters in position 0-1: ordinal not in range(128) 当我们尝试将中文字符以 ASCII 方式编码时，报出了 UnicodeEncodeError。这个原因很明显，ASCII 编码方式并没有定义中文字符，无法对中文字符进行编码。 碰到这种问题，可以使用 errors 参数进行处理：12&gt;&gt;&gt; '中文'.encode('ascii', errors='ignore')b'' 我们将错误的编码忽略掉，最后得到空的字节序列。通常，这样做是非常不妥的。 errors 参数还有许多可选项，大家可以自行探索。 比较妥当的解决方案，当然是选择合适的编码方式。这个需要根据具体情况而定，相信，理解了这个错误发生的原因，解决起来就很轻松啦！ 3.2 UnicodeDecodeError不是每一个字节都包含有有效的 ASCII 字符，也不是每一个字符序列都是有效的 utf-8 。因此把二进制序列转换为文本时，遇到无法转换的字节序列就会抛出 UnicodeDecodeError。 看个例子：12345&gt;&gt;&gt; b = b'\xe4\xb8\xad\xe6\x96\x87'&gt;&gt;&gt; b.decode('ascii')Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt;UnicodeDecodeError: 'ascii' codec can't decode byte 0xe4 in position 0: ordinal not in range(128) b 是字符 ‘中文’ 以 utf-8 方式编码形成的字节序列，属于 Python3 中的 bytes 类型。将该字节序列以 ASCII 方式解码时抛出了 UnicodeDecodeError。 4 总结本文详细阐述了编码的原理及各种问题发生的原因，相信搞懂这些，以后碰到编码问题就再也不用求助 Google 了。知道了问题发生的原因，我们自己就能迅速解决了！ 参考文献廖雪峰的官方网站-字符串和编码大道至简：史上最易懂的『乱码』解决方案《流畅的Python》]]></content>
      <tags>
        <tag>编程语言 编码</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习算法系列（1）- 逻辑斯蒂回归]]></title>
    <url>%2F2018%2F01%2F21%2F%E6%B5%85%E8%B0%88%E9%80%BB%E8%BE%91%E6%96%AF%E8%92%82%E5%9B%9E%E5%BD%92%2F</url>
    <content type="text"><![CDATA[转载请注明出处：https://zhuyuhe.github.io/2018/01/21/%E6%B5%85%E8%B0%88%E9%80%BB%E8%BE%91%E6%96%AF%E8%92%82%E5%9B%9E%E5%BD%92/ 博主即将开始求职之旅，于是搭了这个博客，将备战春招和秋招的复习笔记记录于此。此系列是对机器学习算法理论的复习，力求搞懂算法的来龙去脉，公式繁多，废话较少。以后有时间会在此基础上进行完善，力求通俗易懂。如有不足之处，还请各位读者指正！ 1.从广义线性模型到sigmoid函数首先，LR是二分类模型，我们假设二分类问题服从伯努利分布。即其概率分布为： P(y;\phi)= \phi^y (1-\phi)^{1-y} = exp(ylog\phi + (1-y)log(1-\phi))=exp(ylog\frac{\phi}{1-\phi} + log(1-\phi))而伯努利分布属于指数蔟分布，即其概率分布可以写成如下形式： P(y;\eta)= b(y)exp(\eta^TT(y)-\alpha(\eta))其中，$\eta$为自然参数。广义线性模型认为输入x与自然参数$\eta$为线性关系，即$\eta = \theta^TX$。对比上述两式可以得到： b(y) = 1T(y) = y\eta = log\frac{\phi}{1-\phi}所以有 \theta^TX = \eta = log\frac{\phi}{1-\phi}得到 \phi = \frac{1}{1+e ^ {-\theta^TX}}这里的$\phi$就是伯努利分布中的P(Y=1|X)而LR中的模型输出 h(x) = E(y|x) = 0 · P(Y=0|X) + 1 · P(Y=1|X) = \frac{1}{1+e ^ {-\theta^TX}}这就推导出了逻辑斯蒂回归模型。也解释了为什么LR要用sigmoid函数，因为我们从广义线性模型出发，推导出的LR模型刚好就是sigmoid函数形式的。并且sigmoid有很多良好的数学性质： 连续可导 值域为（0,1）,给了模型可解释性，即将输出结果解释为对应于该分类的概率 下一节我们将从推导出的lr模型出发，去看看它的损失函数是怎么来的。 2. 从极大似然估计到损失函数给定一组数据，我们需要用这组数据去找到最好的参数$\theta$。我们认为使观测结果（即现有的数据）出现的概率最大的参数$\theta$就是最优的参数。这种思想就是极大似然估计。假设这组数据独立同分布，其联合概率可以写成各样本出现概率的乘积。即 L(\theta) = \prod_{i=1} ^ m P_i = \prod_{i=1} ^ m {P(y ^ {(i)}=1 | X ^ {(i)})}^{y ^ {(i)}}{P(y ^ {(i)}=0 | X ^ {(i)})} ^ {1-y ^ {(i)}}以上函数称为似然函数。我们的目标便是最大化似然函数，即找到使联合概率（也就是似然函数）最大的参数$\theta$。为了方便求解，对似然函数取对数，得到对数似然函数： log L(\theta) = \sum_{i=1} ^ m y ^ {(i)}log \frac{1}{1+e ^ {-\theta ^ TX}} + (1-y ^ {(i)})log \frac{1}{1+e ^ {\theta ^ TX}}对对数似然函数取负号，求平均，就得到了LR模型的损失函数： J(\theta) = -\frac{1}{m}\sum_{i=1}^m (y ^ {(i)}log \frac{1}{1+e ^ {-\theta^TX}} + (1-y^{(i)})log \frac{1}{1+e ^ {\theta^TX}})现在我们的目标就就很明确了：优化$J(\theta)$，找出使$J(\theta)$最小的参数$\theta$，就是我们认为的最优的$\theta$。 3. 梯度下降法梯度下降法是常用的优化算法之一。针对LR，梯度下降法的过程如下：首先，有 J(\theta) = -\frac{1}{m}\sum_{i=1}^m (y ^ {(i)}log h(x^{(i)}) + (1-y^{(i)})log (1-h(x^{(i)}))其中$h(x) = \frac{1}{1+e ^ {-\theta^TX}}$接下来求损失函数$J(\theta)$对参数$\theta$的梯度： \frac{\partial J(\theta)}{\partial \theta} = - \frac{1}{m}\sum_{i=1}^m[\frac{y ^ {(i)}}{h(x^{(i)})}·\frac{\partial h(x^{(i)})}{\partial \theta} - \frac{1- y ^ {(i)}}{1 - h(x^{(i)})}·\frac{\partial h(x^{(i)})}{\partial \theta}] = -\frac{1}{m} \sum_{i=1}^m\frac{\partial h(x^{(i)})}{\partial \theta}·\frac{y^{(i)} - h(x^{(i)})}{ h(x^{(i)})(1 - h(x^{(i)}))}将h(x)带入上式： \frac{\partial J(\theta)}{\partial \theta} = \frac{1}{m}\sum_{i=1}^m( h(x^{(i)}) - y^{(i)})·x ^ {(i)}得到迭代公式： \theta := \theta - \alpha\frac{1}{m}\sum_{i=1}^m( h(x^{(i)}) - y^{(i)})·x ^ {(i)}$\alpha$和$\frac{1}{m}$为常数，因此将其合并，得到： \theta := \theta - \alpha\sum_{i=1}^m( h(x^{(i)}) - y^{(i)})·x ^ {(i)}$\alpha$称为learning rate。注意，公式中的变量均为向量形式。至此，我们得到了参数$\theta$的迭代公式，不断迭代直到收敛或损失函数变化很小，我们就得到了最优参数$\theta$ 4. 正则化4.1 假设空间与奥卡姆剃刀原理给定我们一组数据，我们认为这组数据为观测结果。符合这组观测结果的假设有很多。我们可以把学习过程看做在所有假设组成的空间中进行搜索的过程。在这个搜索过程中，我们可能会找到许多满足这组观测数据的假设。比如，坐标系下两点，我们可以用直线拟合，也可以用二次曲线拟合等等。当出现两个模型均很好地符合当前数据时，我们有一个选择模型的指导原则： 奥卡姆剃刀原理：这是一种常用的、自然科学研究中最基本的原则，即“若有多个假设与观察一致，则选最简单的那个” 因此，我们需要在模型的学习过程中，加入这个指导原则。这个过程，就叫正则化。又叫结构风险最小化。 4.2 L0/L1/L2正则化一般，我们认为，参数越少，模型越简单。理想情况下，我们可以在损失函数中加入（不为0的）参数个数来惩罚模型复杂度。即 J(\theta) = -\frac{1}{m}\sum_{i=1}^m (y ^ {(i)}log h(x^{(i)}) + (1-y^{(i)})log (1-h(x^{(i)})) + \lambda\sum_{i=1}^V1（\theta_i）其中，V为参数数量，并且 \begin{equation} 1(x) = \begin{cases} 0, & \text{if x = 0}\newline 1, & \text{if x != 0} \end{cases} \end{equation}这种方法叫做L0正则化。但实际中并不会使用L0正则化，因为很难求解。因此实际中往往使用L1或L2正则化来替代L0正则化。L1正则化为： \lambda||\theta||_1 = \lambda \sum _{i=1}^V|\theta|L2正则化为： \lambda||\theta||_2 = \lambda \sqrt{\sum _ {i=1}^V \theta_i ^ 2}L1正则化和L2正则化都对参数的大小进行了惩罚。这里先说明一个问题： 为什么认为参数越小模型复杂度也越小呢？因为越复杂的模型，越是尝试对所有样本进行拟合，包括一些异常点。这会导致模型在较小的输入区间内，产生较大的输出波动。较大的波动代表着这个区间内导数大，而只有较大的参数才会产生较大的导数。因此参数越大，我们认为模型越复杂。 在L1和L2正则化的选择中，我们需要知道：L1正则化会产生更加稀疏的解，即求得的参数中会有更多的0；L2正则化会产生更多非0但值较小的参数。 也就是说，L1正则化会过滤掉一些无用特征（参数为0，特征就不起作用了），因此L1正则化也是一种特征选择方法。只不过与我们平时手动选择特征不同的是，L1正则化是一种嵌入式地特征选择方法，其特征选择过程与模型的训练过程融为一体，同时完成了。 所以，当所有特征中只有少部分起作用，而我们人工无法辨别时，可以用L1正则化。当大部分特征都能起作用时，使用L2正则化也许更合适。 5. 性能度量模型训练完毕后，我们需要指标来评价其性能。在回归任务中，常用的性能度量是“均方误差”： \frac{1}{m}\sum_{i=1}^m(h(x ^ {(i)}) - y^{(i)}) ^ 2二分类常用的性能度量为查准率、查全率与F1。对于二分类问题，可将样本根据其真实类别与预测类别的组合划分为如下情况： 真实情况 预测情况 正例 反例 正例 TP(真正例) FN(假反例) 反例 FP(假正例) TN(真反例) 查准率P与查全率分别定义为： P = \frac{TP}{TP+FP}R = \frac{TP}{TP+FN}直观上理解，查准率就是我们预测的正例中，有多大比例预测正确了；查全率则是，在原始样本的所有正例中，我们有多大比例预测正确了。 查准率与查全率是一对相互矛盾的指标。一般来说，查准率高时，查全率往往偏低；查全率高时，查准率往往偏低。通常两个模型的查准率与查全率无法比较孰优孰劣时，我们应该综合考虑这两个指标。常用的是F1度量： F1 = \frac{2·P·R}{P+R} = \frac{2·TP}{样例总数+TP-TN}在平时的应用过程中，可以根据实际任务对查准率或查全率的要求来改变权重，来获取更一般的F度量。 写在后面本文从广义线性模型出发，推导了LR模型的产生，损失函数的建立，如何去优化损失函数，正则化以及性能度量。希望能够帮助到大家。由于所学粗浅，文中如有错误或不足，还请各位读者批评指正，感激不尽！我的邮箱：3120104930@zju.edu.cn]]></content>
      <tags>
        <tag>ML</tag>
      </tags>
  </entry>
</search>
