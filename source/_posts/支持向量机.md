---
title: 机器学习算法系列（2）- 支持向量机
date: 2018-01-25 21:29:44
tags:
---

<!-- more -->
# 前言
这节来讲一讲支持向量机（Support Vector Machine, SVM）这个难啃的骨头。照例，我们从以下路线来梳理整个算法的脉络：
模型来源 —> 损失函数 —> 优化损失函数 —> 正则化
其实这对应着统计学习方法的三要素：

 - 模型：就像上一篇逻辑斯蒂回归模型中，我们从广义线性模型推出了LR的表达式，我们的目标就是学习出这个表达式形式的模型。无穷多种参数的组合导致假设空间包含了无穷多个模型。
 - 策略：有了模型的假设空间，接着需要考虑的是如何从假设空间中选取最优模型。一般我们用损失函数来度量模型预测错误的程度。我们可以通过尽可能选取损失函数最小的模型来得到最优模型。其中，监督学习中有两个基本策略：经验风险最小化和结构风险最小化。
 - 算法：基于数据集，通过什么样的计算方法，根据学习策略去选取最优的模型。这里的算法一般指最优化算法，目标是通过数值计算方法求解损失函数的全局最优值。梯度下降法就是其中之一。


# 间隔与支持向量
给定数据集，分类问题最基本的想法就是基于训练集在样本空间中找到一个划分超平面，将不同类别的样本分开。
但是，这样的超平面可能有很多，我们应该选择哪一个呢？
其实SVM的想法很简单，就是我们要找的直线应该位于两类样本的“正中间”。
![SVM示意图][1]
如上图所示，H2和H3都能完全将两类样本点分开，但我们认为H3优于H2。

因此，我们需要求解的模型就是：
$$y = \omega^Tx + b$$
可以看出，对于黑色样本点，$\omega^T x_i + b > 0$, 并且样本点离超平面越远，该值越大。
对于白色样本点，$\omega^T x_i + b < 0$, 并且样本点离超平面越远，该值越小（绝对值越大）。

## 函数间隔与几何间隔
一般来说，一个点距离超平面的远近可以表示分类预测的确信程度。
在超平面确定的情况下，$|\omega^T x + b|$ 能够相对地表示点x距离超平面的远近，而$\omega^Tx+b$的符号与类标记y的符号是否一致能够表示分类是否正确。因此可以用$y (\omega^Tx + b)$来表示分类的正确度及确信度，这就是函数间隔：
$$\hat{\gamma_i} = y_i(\omega·x_i + b)$$
但函数间隔存在一个问题，当$\omega$和$b$成倍增加为$2\omega$和$2b$时，超平面并未发生改变，而此时的函数间隔却变为了之前的二倍。所以为了规范化间隔及模型参数，我们引入几何间隔：
$$\gamma_i = y_i (\frac{\omega}{||\omega||}·x_i + \frac{b}{||\omega||})$$
其中，$||\omega||$为$\omega$的L2范数。即$||\omega|| = \sum_{i=1}^V \omega_i^2$

## 间隔最大化
回想我们之前提到的SVM的想法，我们要找的直线应该位于两类样本的“正中间”。
也就是说我们要找的直线应该尽可能地离所有样本点远。只需要最大化所有点到直线的间隔的最小值即可。
用数学语言表示出来就是：
$$\max_{\omega, b}  \gamma$$
$$ 
\begin{equation}
\begin{aligned}
c^{j-1}_n=\sum_{l=0}^{D-1}g_lc_{l+2n}^j \newline
d^{j-1}_n=\sum_{l=0}^{D-1}h_lc_{l+2n}^j
\end{aligned}
\end{equation}$$

即，我们希望在所有点的几何间隔大于$\gamma$的情况下，最大化$\gamma$。
根据几何间隔和函数间隔的关系，我们可将问题改写为：
$$\max_{\omega, b} \frac{\hat{\gamma}}{||\omega||}$$



# 对偶问题
TODO
# SMO优化算法
TODO
# 核函数
TODO
# 软间隔与正则化
TODO
# 支持向量回归（SVD）
TODO
# 附：拉格朗日对偶性
TODO


  [1]: ./images/1517231030925.jpg
