
---
title: 浅谈逻辑斯蒂回归
date: 2018-01-21 15:15:46
tags: ML 
---

> 转载请注明出处：
> https://zhuyuhe.github.io/2018/01/21/%E6%B5%85%E8%B0%88%E9%80%BB%E8%BE%91%E6%96%AF%E8%92%82%E5%9B%9E%E5%BD%92/

作为本博客的第一篇文章，本文谈谈逻辑斯蒂回归（Logistic Regression）。
## 1.从广义线性模型到sigmoid函数
首先，LR是二分类模型，我们假设二分类问题服从伯努利分布。即其概率分布为：
$$P(y;\phi)= \phi^y (1-\phi)^{1-y} = exp(ylog\phi + (1-y)log(1-\phi))=exp(ylog\frac{\phi}{1-\phi} + log(1-\phi))$$
而伯努利分布属于指数蔟分布，即其概率分布可以写成如下形式：
$$P(y;\eta)= b(y)exp(\eta^TT(y)-\alpha(\eta))$$
其中，$\eta$为自然参数。
广义线性模型认为输入x与自然参数$\eta$为线性关系，即$\eta = \theta^TX$。
对比上述两式可以得到：
$$b(y)  =  1$$ 
$$T(y) =  y $$
$$\eta  =  log\frac{\phi}{1-\phi} $$


所以有
$$\theta^TX = \eta = log\frac{\phi}{1-\phi}$$

得到
$$\phi = \frac{1}{1+e ^ {-\theta^TX}}$$

这里的$\phi$就是伯努利分布中的P(Y=1|X)
而LR中的模型输出
$$h(x) = E(y|x) = 0 · P(Y=0|X) + 1 · P(Y=1|X) = \frac{1}{1+e ^ {-\theta^TX}}$$



这就推导出了逻辑斯蒂回归模型。也解释了为什么LR要用sigmoid函数，因为我们从广义线性模型出发，推导出的LR模型刚好就是sigmoid函数形式的。
并且sigmoid有很多良好的数学性质：

 - 连续可导
 - 值域为（0,1）,给了模型可解释性，即将输出结果解释为对应于该分类的概率
 
 
下一节我们将从推导出的lr模型出发，去看看它的损失函数是怎么来的。

## 2. 从极大似然估计到损失函数
给定一组数据，我们需要用这组数据去找到最好的参数$\theta$。
我们认为使观测结果（即现有的数据）出现的概率最大的参数$\theta$就是最优的参数。
这种思想就是极大似然估计。
假设这组数据独立同分布，其联合概率可以写成各样本出现概率的乘积。即


$$L(\theta) = \prod_{i=1} ^ m P_i = \prod_{i=1} ^ m {P(y ^ {(i)}=1 | X ^ {(i)})}^{y ^ {(i)}}{P(y ^ {(i)}=0 | X ^ {(i)})} ^ {1-y ^ {(i)}}$$



以上函数称为似然函数。
我们的目标便是最大化似然函数，即找到使联合概率（也就是似然函数）最大的参数$\theta$。
为了方便求解，对似然函数取对数，得到对数似然函数：


$$log L(\theta) = \sum_{i=1} ^ m y ^ {(i)}log \frac{1}{1+e ^ {-\theta ^ TX}} + (1-y ^ {(i)})log \frac{1}{1+e ^ {\theta ^ TX}}$$


对对数似然函数取负号，求平均，就得到了LR模型的损失函数：


$$J(\theta) = -\frac{1}{m}\sum_{i=1}^m (y ^ {(i)}log \frac{1}{1+e ^ {-\theta^TX}} + (1-y^{(i)})log \frac{1}{1+e ^ {\theta^TX}})$$


现在我们的目标就就很明确了：
优化$J(\theta)$，找出使$J(\theta)$最小的参数$\theta$，就是我们认为的最优的$\theta$。

## 3. 梯度下降法
TODO
## 4. 正则化
TODO
## 5. 评价指标
TODO
## 写在后面
本文从广义线性模型出发，推导了LR模型的产生，损失函数的建立，以及如何去优化损失函数。希望能够帮助到大家。由于所学粗浅，文中如有错误或不足，还请各位读者批评指正，感激不尽！
我的邮箱：3120104930@zju.edu.cn
